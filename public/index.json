[
{
	"uri": "http://localhost:1313/aws_ojt_report-TranThiThuHa/3-blogstranslated/3.1-blog1/",
	"title": "Blog 1",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://localhost:1313/aws_ojt_report-TranThiThuHa/3-blogstranslated/3.2-blog2/",
	"title": "Blog 2",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://localhost:1313/aws_ojt_report-TranThiThuHa/3-blogstranslated/3.3-blog3/",
	"title": "Blog 3",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://localhost:1313/aws_ojt_report-TranThiThuHa/3-blogstranslated/3.4-blog4/",
	"title": "Blog 4",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://localhost:1313/aws_ojt_report-TranThiThuHa/3-blogstranslated/3.5-blog5/",
	"title": "Blog 5",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://localhost:1313/aws_ojt_report-TranThiThuHa/3-blogstranslated/3.6-blog6/",
	"title": "Blog 6",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://localhost:1313/aws_ojt_report-TranThiThuHa/5-workshop/5.3-s3-vpc/5.3.1-create-gwe/",
	"title": "Create a gateway endpoint",
	"tags": [],
	"description": "",
	"content": " Open the Amazon VPC console In the navigation pane, choose Endpoints, then click Create Endpoint: You will see 6 existing VPC endpoints that support AWS Systems Manager (SSM). These endpoints were deployed automatically by the CloudFormation Templates for this workshop.\nIn the Create endpoint console: Specify name of the endpoint: s3-gwe In service category, choose AWS services In Services, type s3 in the search box and choose the service with type gateway For VPC, select VPC Cloud from the drop-down. For Configure route tables, select the route table that is already associated with two subnets (note: this is not the main route table for the VPC, but a second route table created by CloudFormation). For Policy, leave the default option, Full Access, to allow full access to the service. You will deploy a VPC endpoint policy in a later lab module to demonstrate restricting access to S3 buckets based on policies. Do not add a tag to the VPC endpoint at this time. Click Create endpoint, then click x after receiving a successful creation message. "
},
{
	"uri": "http://localhost:1313/aws_ojt_report-TranThiThuHa/4-eventparticipated/4.1-event1/",
	"title": "Event 1",
	"tags": [],
	"description": "",
	"content": "Event Report: Vietnam Cloud Day 2025 - Ho Chi Minh City Connect Edition for Builders Event Overview Event Name: Vietnam Cloud Day 2025: Ho Chi Minh City Connect Edition for Builders\nDate \u0026amp; Time: Thursday, September 18, 2025, 9:00 AM - 5:00 PM\nLocation: Ho Chi Minh City, Vietnam\nMy Role: Attendee\nEvent Purpose Vietnam Cloud Day 2025 served as a comprehensive platform to explore the digital transformation landscape in Vietnam through cloud technologies. The event brought together industry leaders, technical experts, and builders to discuss:\nAWS Vision \u0026amp; Strategy: Understanding AWS\u0026rsquo;s roadmap for Cloud and Generative AI in Vietnam Enterprise Insights: Learning from real-world modernization experiences shared by major Vietnamese enterprises Technical Deep Dives: Gaining practical knowledge through breakout sessions covering Migration, Modernization, Security, and GenAI-powered development Networking Opportunities: Connecting with peers, experts, and potential collaborators in the cloud ecosystem Distinguished Speakers The event featured an impressive lineup of industry leaders and technical experts:\nEric Yeo – Country General Manager, AWS Dr. Jens Lottner – CEO, Techcombank Ms. Trang Phung – CEO \u0026amp; Co-Founder, U2U Network Jaime Valles – VP, GM Asia Pacific and Japan, AWS Jeff Johnson – MD, ASEAN, AWS Vu Van – CEO, ELSA Corp Nguyen Hoa Binh – Chairman, Nexttech Group Dieter Botha – CEO, TymeX Hung Nguyen Gia – Head of Solutions Architect, AWS Phuc Nguyen – Solutions Architect, AWS Alex Tran – AI Director, OCB Additionally, numerous experts from organizations including LPBank, Ninety Eight, Techcombank, and other leading companies contributed their insights throughout the day.\nKey Highlights \u0026amp; Sessions Opening Keynote: AWS Strategic Vision The opening session set the tone for the entire event, with AWS leadership presenting their strategic vision for Vietnam\u0026rsquo;s cloud transformation. The keynote emphasized:\nOpportunities for young builders and developers in the cloud ecosystem AWS\u0026rsquo;s commitment to supporting Vietnam\u0026rsquo;s digital transformation journey The convergence of Cloud and Generative AI as a catalyst for innovation Enterprise Transformation Stories Two compelling case studies stood out:\nTechcombank\u0026rsquo;s Journey:\nReal-world insights from one of Vietnam\u0026rsquo;s largest banks Challenges and solutions in migrating critical banking systems to the cloud Lessons learned from large-scale cloud transformation U2U Network\u0026rsquo;s Experience:\nA startup perspective on leveraging cloud technologies How cloud infrastructure enabled rapid scaling and innovation Practical approaches to cloud adoption for growing companies GenAI Revolution Panel Discussion A thought-provoking panel featuring CEOs, CTOs, and AI directors explored:\nThe role of AI in shaping business strategy Innovation opportunities with Generative AI Balancing technological advancement with practical business needs Diverse perspectives on AI adoption across different industries Migration \u0026amp; Modernization Track This technical track provided deep insights into cloud migration strategies:\nLarge-Scale Workload Migration:\nCase studies on migrating enterprise applications to AWS Best practices for minimizing downtime and risk Strategies for handling complex, mission-critical systems Amazon Q Developer Demonstration:\nLive demonstration of automated software development lifecycle (SDLC) capabilities Code generation, transformation, and modernization features Integration with existing development workflows Potential impact on developer productivity Application Modernization:\nApproaches to modernizing legacy applications Security considerations in the AI era Balancing modernization with operational stability Technical Deep Dive Sessions VMware to AWS Modernization Roadmap:\nMigration strategies from VMware to AWS services (EKS, RDS, serverless) Technical considerations and best practices Cost optimization and performance improvements Security at Scale:\nComprehensive security approaches from development to production How GenAI analysis enhances security monitoring and threat detection DevSecOps integration strategies Real-world security implementation patterns Key Learnings \u0026amp; Insights Strategic Thinking Business-First Philosophy: Technology selection must always begin with understanding business requirements and objectives. The most advanced technology is useless if it doesn\u0026rsquo;t solve real business problems.\nPhased Migration Approach: Successful cloud migration requires a well-defined, incremental roadmap. Attempting to migrate everything at once is a recipe for failure. Each phase should build upon lessons learned from previous phases.\nSecurity by Design: Security cannot be an afterthought. It must be integrated from the initial design phase through to production deployment. This \u0026ldquo;security by design\u0026rdquo; approach prevents costly remediation later.\nTechnical Insights Amazon Q Developer\u0026rsquo;s Potential: The demonstration revealed significant potential for accelerating SDLC processes. From code generation to automated testing and documentation, GenAI tools can substantially improve developer productivity when properly integrated.\nModernization Frameworks: The event reinforced that there\u0026rsquo;s no one-size-fits-all approach to modernization. Each organization must assess its unique context, constraints, and goals to determine the appropriate modernization strategy.\nCloud-Native Architecture: The discussions highlighted the importance of designing for cloud-native principles from the start, rather than simply \u0026ldquo;lifting and shifting\u0026rdquo; existing architectures.\nIndustry Perspectives GenAI\u0026rsquo;s Dual Nature: While GenAI offers tremendous potential, success ultimately depends on people, strategy, and proper implementation. Technology alone is not a silver bullet.\nContinuous Learning Mindset: The rapid evolution of cloud and AI technologies requires builders to maintain a growth mindset and remain open to experimenting with new tools and approaches.\nCloud + AI as Foundation: The combination of cloud infrastructure and AI capabilities will become the fundamental platform for business growth in Vietnam\u0026rsquo;s digital economy.\nApplication to My Work Based on the insights gained from Vietnam Cloud Day 2025, I plan to apply the following in my internship and future projects:\nImmediate Actions GenAI Use Case Exploration: Organize brainstorming sessions with my team to identify practical GenAI applications that can enhance our current projects. Focus on areas where AI can add real value rather than implementing it for the sake of novelty.\nAmazon Q Developer Experimentation: Integrate Amazon Q Developer into my development workflow to evaluate its impact on productivity. Start with smaller tasks like code documentation and gradually expand to more complex use cases.\nPhased Migration Planning: When working on cloud projects, always develop a phased migration roadmap. Break down large migrations into manageable increments with clear success criteria for each phase.\nLong-Term Strategies DevSecOps Integration: Incorporate security practices throughout the entire development lifecycle. This means considering security implications during design, implementation, testing, and deployment phases.\nContinuous Learning: Stay updated with the latest AWS services and GenAI tools. Regularly experiment with new technologies in sandbox environments to understand their capabilities and limitations.\nBusiness-Technology Alignment: Always start project discussions by understanding business objectives before diving into technical solutions. This alignment ensures that technical efforts contribute to real business value.\nPersonal Experience \u0026amp; Reflection Attending Vietnam Cloud Day 2025 was a transformative experience, particularly as it was my first time participating in such a large-scale cloud technology event. Several aspects left a lasting impression:\nLearning from Industry Leaders The opportunity to hear directly from CEOs and CTOs of major Vietnamese enterprises provided invaluable insights that go beyond technical documentation. Their presentations revealed:\nReal-world challenges that organizations face during cloud transformation Practical solutions that have been tested in production environments Local context specific to Vietnam\u0026rsquo;s business and regulatory environment Strategic thinking behind technology decisions at the executive level These perspectives helped me understand that cloud adoption is not just a technical exercise but a strategic business transformation.\nThe GenAI Panel: Diverse Perspectives The GenAI panel discussion was particularly enlightening because it showcased diverse viewpoints rather than a single narrative. The panelists represented different industries, company sizes, and use cases, which highlighted that:\nAI adoption strategies vary significantly based on industry and business model Success requires alignment between AI initiatives and overall business strategy There\u0026rsquo;s no universal \u0026ldquo;right way\u0026rdquo; to implement GenAI—context matters The human element (strategy, people, culture) is as important as the technology itself Amazon Q Developer: A Glimpse into the Future The live demonstration of Amazon Q Developer provided a clear picture of how GenAI can transform software development. Seeing the tool in action made the potential benefits tangible:\nAccelerated development cycles through automated code generation Improved code quality through AI-assisted reviews and suggestions Enhanced documentation that stays synchronized with code changes Reduced cognitive load on developers for routine tasks This demonstration reinforced my belief that GenAI tools will become essential parts of the developer toolkit, similar to how IDEs and version control systems became standard.\nNetworking Value Beyond the formal sessions, the networking opportunities were equally valuable. Conversations with other builders, solutions architects, and industry professionals provided:\nPeer learning from others facing similar challenges Different perspectives on common problems Potential collaboration opportunities for future projects Industry insights that aren\u0026rsquo;t available in documentation or tutorials Key Takeaways for My Career Technology is a Means, Not an End: The most successful cloud transformations start with business objectives, not technology features. This principle will guide my approach to all future projects.\nMigration Requires Patience and Planning: Rushing cloud migrations leads to problems. A well-planned, phased approach may take longer initially but results in more sustainable, successful outcomes.\nGenAI is Powerful but Requires Strategy: The potential of GenAI is undeniable, but realizing that potential requires thoughtful strategy, proper implementation, and alignment with business goals.\nContinuous Learning is Essential: The cloud and AI landscapes evolve rapidly. Staying relevant requires a commitment to continuous learning and experimentation.\nBuilders Must Stay Open-Minded: The best solutions often come from unexpected combinations of technologies or approaches. Maintaining an open, experimental mindset is crucial for innovation.\nEvent Photos Add your event photos here\nVietnam Cloud Day 2025 was more than just a technology conference—it was a comprehensive learning experience that combined strategic insights, technical depth, and practical applications. The event reinforced my passion for cloud computing and provided a clearer vision of how Cloud + AI will shape Vietnam\u0026rsquo;s digital future. The connections made and knowledge gained will undoubtedly influence my approach to cloud engineering throughout my career.\n"
},
{
	"uri": "http://localhost:1313/aws_ojt_report-TranThiThuHa/4-eventparticipated/4.2-event2/",
	"title": "Event 2",
	"tags": [],
	"description": "",
	"content": "Event Report: AI-Driven Development Workshop – Shaping the Future of Development Event Overview Event Name: AI-Driven Development Workshop – Shaping the Future of Development\nDate \u0026amp; Time: Friday, October 3, 2025, 14:00 – 16:30\nVenue: AWS Event Hall, 26th Floor – Bitexco Tower, Ho Chi Minh City, Vietnam\nMy Role: Attendee\nEvent Information Speakers Mr. Toan Huynh – Presenter on AI-Driven Development framework and future trends Ms. My Nguyen – Presenter and demonstrator of Kiro IDE Extension Event Coordinators Diem My Dai Truong Dinh Nguyen Event Objectives The workshop was designed to provide participants with:\nUnderstanding AI-Orchestrated Development: Insights into new trends where AI orchestrates the entire software development process AI-Driven Development Lifecycle (AI-DLC) Framework: A comprehensive model that integrates AI throughout all phases of software development Practical Tool Demonstrations: Hands-on experience with Amazon Q Developer and Kiro IDE Extension Productivity Analysis: Understanding how AI enhances productivity, accelerates development speed, and improves software quality Future Vision: Guidance on the evolving role of AI in modern software development practices Content \u0026amp; Key Highlights 1. Opening Session: The Future of Software Development Mr. Toan Huynh opened the workshop with a compelling presentation titled \u0026ldquo;Shaping the Future of Development.\u0026rdquo; The session highlighted a fundamental paradigm shift in software development:\nFrom Traditional to AI-Orchestrated Development:\nThe evolution from conventional development models to AI-orchestrated approaches AI\u0026rsquo;s expanding role from simple code suggestions to orchestrating entire development workflows How AI now supports processes spanning from initial planning and design through programming, testing, and deployment This opening set the stage for understanding AI not as a replacement for developers, but as an intelligent orchestrator that enhances human capabilities.\n2. Challenges in Current Development Models The presentation critically examined existing AI integration approaches:\nLimitations Identified:\nAI-Assisted Development: While helpful, this model lacks stability and consistency AI-Managed Development: Difficult to explain and doesn\u0026rsquo;t fully deliver on the promise of \u0026ldquo;AI acceleration\u0026rdquo; The Solution - AI-Driven Development (AI-DD):\nA balanced approach that combines automation with human oversight Maintains developer control while leveraging AI\u0026rsquo;s capabilities Addresses the shortcomings of previous models through intelligent orchestration 3. AI-Driven Development Lifecycle (AI-DLC) Model The core of the workshop focused on the AI-DLC framework, which represents three evolutionary levels:\nLevel 1: AI-Assisted Development\nAI provides support functions: code generation, intelligent suggestions, syntax checking Developers maintain primary control and decision-making authority Level 2: AI-Driven Development\nAI actively participates in architecture design, planning, and decision support More collaborative relationship between AI and developers AI contributes to strategic decisions while humans retain final authority Level 3: AI-Managed Development\nAI automatically orchestrates development processes Human approval required at key decision points AI acts as an intelligent coordinator managing workflow efficiency Key Principle: In all levels, AI serves as an \u0026ldquo;intelligent coordinator\u0026rdquo; while developers retain decision-making authority and responsibility for output verification.\n4. Benefits of AI in Software Development The presentation \u0026ldquo;AI in Development – Outcomes\u0026rdquo; outlined seven significant benefits of AI integration:\nPredictability: Maintains consistent progress and enables businesses to accurately predict release schedules Velocity: Accelerates time-to-market for new ideas and features Quality: Reduces errors and improves overall software stability Innovation: Inspires new ideas and creative approaches to problem-solving Developer Engagement: Increases programmer interest and work efficiency Customer Satisfaction: Enhances user experience and overall satisfaction Productivity: Reduces development time while increasing overall efficiency 5. Software Development Lifecycle (SDLC) Analysis The workshop analyzed where developers spend their time in the SDLC through the diagram \u0026ldquo;Where are developers spending time in the SDLC?\u0026rdquo; The lifecycle includes:\nExplore \u0026amp; Plan: Initial requirements gathering and planning Create: Development and implementation Test \u0026amp; Secure: Quality assurance and security testing Review \u0026amp; Deploy: Code review and deployment processes Maintain, Transform \u0026amp; Modernize: Ongoing maintenance and system evolution AI\u0026rsquo;s Impact: AI significantly shortens time in labor-intensive stages like testing, deployment, and maintenance through automation and intelligent analysis, allowing developers to focus on higher-value activities.\n6. Standard AI-DLC Process The \u0026ldquo;AI-DLC Standard Development Workflow\u0026rdquo; slide presented four core steps:\nStep 1: Requirement\nProduct Owner collects and analyzes requirements AI assists in requirement analysis and documentation Step 2: Design\nSoftware Architect designs architecture, processes, and APIs AI supports architectural decision-making and design validation Step 3: Implementation\nSoftware Engineer implements, tests, and integrates AI assists in code generation, testing, and integration Step 4: Deployment\nSystem deployment and monitoring AI supports deployment automation and monitoring setup Key Feature: AI supports all four stages, making the entire process more consistent, clear, and efficient.\n7. AI-DLC Process Characteristics According to the \u0026ldquo;Key Workflow Features\u0026rdquo; slide, the AI-DLC process has four main characteristics:\nRole Separation:\nClear separation between business, architecture, and implementation roles Each role has defined responsibilities and boundaries AI-Enhanced:\nEach role has its own AI context tailored to specific expertise AI personas customized for Product Manager, Architect, and Developer needs Iterative:\nContinuous feedback loops between stages Enables rapid iteration and improvement Template-Driven:\nStandardizes outputs with unified AI-DLC templates Ensures consistency across projects and teams 8. AI Integration in Each Development Stage The \u0026ldquo;Each Stage\u0026rdquo; slide detailed how AI is integrated throughout development:\nSpecific Context:\nEach role (PM, Architect, Developer) has its own AI persona AI context is tailored to role-specific needs and expertise Clear Inputs/Outputs:\nClearly defined inputs and outputs between steps Ensures smooth handoffs and reduces miscommunication Interactive Approach:\nSupports collaboration and parallel feedback between AI and humans Enables real-time interaction and iterative refinement Documentation:\nContinuously updates documents like prompts.md, dashboard.md Tracks progress and improves AI models over time Maintains comprehensive project documentation 9. Live Demonstrations Amazon Q Developer (Presented by Mr. Toan Huynh) Key Features:\nAI assistant seamlessly integrated into popular IDEs (VS Code, Cloud9, etc.) Automatically generates code, tests, and documentation Suggests AWS architecture patterns and best practices Supports updating prompt.md and automating CI/CD processes Live Demo Highlights:\nCreating comprehensive development plans through AI commands Generating user stories automatically Managing projects with AI assistance Demonstrating how AI accelerates the entire development workflow Impact: The demonstration clearly showed how Amazon Q Developer can save significant time while reducing errors and supporting automated testing and deployment.\nKiro IDE Extension (Presented by Ms. My Nguyen) Key Features:\nIDE extension for creating and managing specification documents Supports requirements.md, design.md, and tasks.md generation AI can create feature descriptions and define API flows Generates backend code automatically Live Demo Highlights:\nCreating a complete Chat application with user authentication Demonstrating login, registration, and token handling functionality Showing the process from specification to working code Illustrating AI\u0026rsquo;s comprehensive support from requirements to implementation Impact: The demonstration showcased how AI can support programmers from writing specifications through automated code generation, significantly reducing development time.\nKey Takeaways Strategic Insights AI as a Smart Teammate: AI doesn\u0026rsquo;t replace programmers but becomes an intelligent teammate supporting them at every development stage. This partnership model maximizes both human creativity and AI efficiency.\nStandardization Through AI-DLC: The AI-DLC model helps standardize development processes, ensuring transparency and consistency across software development projects. This standardization is crucial for enterprise-scale development.\nComprehensive AI Support: Modern AI tools like Amazon Q Developer and Kiro IDE demonstrate AI\u0026rsquo;s ability to support the entire development lifecycle, from initial requirements through deployment and maintenance.\nDevSecOps Integration: Integrating AI into DevSecOps is an inevitable trend to achieve high productivity while ensuring security. AI can enhance security practices throughout the development process.\nTechnical Learnings Tool-Specific Benefits:\nAmazon Q Developer: Saves time, reduces errors, supports automated testing and deployment Kiro IDE: Provides comprehensive support from specification to code generation Process Improvement: The AI-DLC framework provides a structured approach to integrating AI into existing development workflows without disrupting established practices.\nQuality Enhancement: AI integration leads to improved code quality, reduced errors, and better documentation, ultimately resulting in higher customer satisfaction.\nPractical Applications Based on the workshop insights, I plan to apply the following in my work:\nImmediate Actions Amazon Q Developer Integration:\nImplement Amazon Q Developer to automate testing in internal projects Use AI assistance for creating comprehensive documentation Explore automated code generation for repetitive tasks Kiro IDE Adoption:\nUse Kiro IDE to standardize specification processes Leverage AI for rapid backend code generation Improve documentation consistency across projects AI-Driven Sprint Organization:\nOrganize AI-driven sprints to evaluate AI effectiveness in software development teams Measure productivity improvements and quality enhancements Gather feedback for continuous improvement Long-Term Strategies AI-Assisted Code Review:\nDeploy an \u0026ldquo;AI-Assisted Code Review\u0026rdquo; model to optimize product quality Combine human expertise with AI analysis for comprehensive code reviews Establish best practices for AI-enhanced code review processes Process Standardization:\nAdopt AI-DLC templates to standardize development workflows Create role-specific AI personas for different team members Establish clear input/output definitions between development stages Continuous Learning:\nStay updated with evolving AI development tools Experiment with new AI capabilities as they become available Share learnings with team members to build collective AI expertise Personal Reflection Attending the \u0026ldquo;AI-Driven Development Workshop\u0026rdquo; was an in-depth and highly practical experience that significantly expanded my understanding of AI\u0026rsquo;s role in modern software development.\nLearning from Expert Presentations Mr. Toan Huynh\u0026rsquo;s Strategic Vision: Mr. Toan Huynh\u0026rsquo;s presentation on AI\u0026rsquo;s role in the future of software development was highly directional and thought-provoking. His explanation of the relationship between AI and humans in the development process helped me understand that:\nAI is not a threat to developers but a powerful ally The key to success is finding the right balance between automation and human control AI-driven development requires new ways of thinking about software development workflows The future belongs to developers who can effectively collaborate with AI tools Ms. My Nguyen\u0026rsquo;s Practical Demonstration: Ms. My Nguyen\u0026rsquo;s live demonstration with Kiro IDE was a true highlight of the workshop. Her practical approach clearly showed:\nHow AI can support programmers from the very beginning of a project The seamless transition from specification to working code Real-world applications of AI in actual development scenarios The tangible benefits of AI integration in daily development work Key Realizations AI as a Strategic Platform: The event helped me deeply realize that AI is not just a support tool but a strategic platform helping businesses innovate faster, more accurately, and more sustainably. This shift in perspective is crucial for understanding AI\u0026rsquo;s true potential.\nThe Human-AI Partnership: The workshop reinforced that the most effective approach combines human creativity, judgment, and strategic thinking with AI\u0026rsquo;s speed, consistency, and analytical capabilities. This partnership model will define the future of software development.\nContinuous Evolution: The three-level AI-DLC model (AI-Assisted → AI-Driven → AI-Managed) shows that AI integration is an evolutionary process. Organizations can start at the appropriate level and gradually advance as they become more comfortable with AI capabilities.\nImpact on My Career This workshop has fundamentally changed how I view AI in software development. I now understand that:\nLearning to work effectively with AI tools is essential for modern developers AI integration requires understanding both technical capabilities and strategic implications The developers who embrace AI collaboration will have significant advantages Continuous learning about AI tools and frameworks is now part of professional development Future Directions The workshop has inspired me to:\nActively explore and experiment with AI development tools Share knowledge about AI-driven development with my team Contribute to establishing best practices for AI integration Stay at the forefront of AI development trends and tools Event Photos Add your event photos here\nThe AI-Driven Development Workshop was a transformative experience that opened my eyes to the future of software development. The combination of strategic vision, practical demonstrations, and hands-on tools provided a comprehensive understanding of how AI is reshaping the development landscape. The insights gained from this workshop will undoubtedly influence my approach to software development and guide my career in the AI-enhanced future of technology.\n"
},
{
	"uri": "http://localhost:1313/aws_ojt_report-TranThiThuHa/4-eventparticipated/4.3-event3/",
	"title": "Event 3",
	"tags": [],
	"description": "",
	"content": "Event Report: AI/ML/GenAI on AWS Workshop Event Overview Event Name: AI/ML/GenAI on AWS Workshop\nDate \u0026amp; Time: Saturday, November 15, 2025, 8:30 AM – 12:00 PM\nLocation: AWS Vietnam Office\nMy Role: Attendee\nEvent Purpose The AI/ML/GenAI on AWS Workshop was meticulously crafted to deliver comprehensive, hands-on experience with AWS\u0026rsquo;s artificial intelligence and machine learning services. The event strategically focused on two critical domains:\nAmazon SageMaker: Exploring traditional machine learning workflows, covering the complete journey from data preparation through model deployment and monitoring Amazon Bedrock: Delving into generative AI applications, examining cutting-edge capabilities for building intelligent, AI-powered solutions The workshop\u0026rsquo;s primary goal was to bridge the gap between theoretical understanding and practical implementation, empowering participants to build, deploy, and manage real-world AI/ML solutions on AWS infrastructure.\nAgenda Overview 8:30 – 9:00 AM | Welcome \u0026amp; Introduction The workshop commenced with an engaging welcome session that established the foundation for an interactive learning experience:\nParticipant Registration \u0026amp; Networking:\nSmooth registration process with distribution of comprehensive workshop materials Valuable networking opportunities connecting with fellow participants, AWS technical experts, and industry professionals Informal conversations about AI/ML interests, experiences, and career aspirations Workshop Overview \u0026amp; Objectives:\nClear presentation of learning objectives and expected outcomes Detailed walkthrough of the day\u0026rsquo;s agenda and session structure Introduction to the AWS AI/ML ecosystem and its growing significance in Vietnam\u0026rsquo;s technology transformation Ice-Breaker Activity:\nCollaborative exercises designed to build connections among participants Group discussions about AI/ML use cases and industry-specific challenges Establishing a collaborative, interactive atmosphere for knowledge sharing AI/ML Landscape in Vietnam:\nOverview of current AI/ML adoption trends across Vietnamese enterprises Market opportunities and unique challenges in the Vietnamese context AWS\u0026rsquo;s strategic role in supporting Vietnam\u0026rsquo;s digital transformation through AI/ML innovation 9:00 – 10:30 AM | AWS AI/ML Services Overview This comprehensive session provided deep technical insights into Amazon SageMaker, AWS\u0026rsquo;s fully integrated machine learning platform.\nAmazon SageMaker – Complete ML Platform Data Preparation and Labeling:\nThe session covered essential data preparation techniques for machine learning success:\nData Cleaning Methods: Understanding systematic approaches to prepare datasets, including handling missing values, identifying and managing outliers, and addressing data quality issues that can impact model performance Feature Engineering Strategies: Learning advanced methods to transform raw data into meaningful, predictive features that enhance model accuracy and generalization Automated Labeling Solutions: Exploring SageMaker Ground Truth capabilities for creating high-quality training datasets with minimal manual intervention, significantly reducing labeling costs and time Scalable Data Pipelines: Understanding how to construct robust, scalable data preprocessing workflows that can handle large-scale datasets efficiently Model Training, Tuning, and Deployment:\nComprehensive coverage of the model development lifecycle:\nDistributed Training Infrastructure: Exploring SageMaker\u0026rsquo;s powerful distributed training capabilities for handling massive datasets and complex deep learning models across multiple instances Automated Hyperparameter Optimization: Learning to leverage SageMaker Automatic Model Tuning to systematically find optimal hyperparameters, reducing manual experimentation time Flexible Deployment Options: Real-time Inference: Deploying models for low-latency predictions using SageMaker endpoints, suitable for interactive applications Batch Inference: Processing large datasets efficiently using batch transform jobs for cost-effective bulk predictions Serverless Inference: Understanding cost-effective deployment options for variable workloads with automatic scaling Model Optimization Techniques: Strategies for reducing model size, improving inference speed, and optimizing costs without sacrificing accuracy Integrated MLOps Capabilities:\nProduction-ready machine learning operations:\nModel Versioning and Registry: Learning about SageMaker Model Registry for comprehensive tracking of model versions, metadata, and lineage Production Monitoring: Understanding how to monitor model performance in production using SageMaker Model Monitor, detecting data drift and performance degradation Automated Retraining Pipelines: Building CI/CD pipelines for continuous model improvement, ensuring models stay current with evolving data patterns Experiment Tracking: Using SageMaker Experiments to systematically track, compare, and analyze different model training runs Model Governance: Best practices for managing the complete model lifecycle and ensuring regulatory compliance Live Demo: SageMaker Studio Walkthrough The demonstration showcased SageMaker Studio, a unified development environment that revolutionizes the machine learning workflow:\nJupyter Notebook Integration:\nSeamless integration with Jupyter notebooks for interactive, exploratory development Collaborative features enabling team-based ML projects with shared workspaces Pre-configured environments with popular ML frameworks (TensorFlow, PyTorch, Scikit-learn) ready to use Experiment Tracking and Model Registry:\nIntuitive visual interface for tracking experiments and comparing model performance metrics Centralized model registry for organizing, cataloging, and managing trained models Easy comparison tools for analyzing different model versions and their performance characteristics Visual Workflow Builder:\nDrag-and-drop interface for building complex MLOps pipelines without extensive coding Visual representation of data processing, training, and deployment workflows Integration with AWS Step Functions for sophisticated workflow orchestration AWS Services Integration:\nSeamless integration with S3 for scalable data storage and retrieval Connection with AWS data processing services (Glue, EMR, Athena) for comprehensive data pipelines Integration with CloudWatch for comprehensive monitoring, logging, and alerting Key Insight: SageMaker Studio eliminates the traditional need to switch between multiple tools, dramatically improving developer productivity and reducing the time from initial concept to deployed, production-ready model.\n10:30 – 10:45 AM | Coffee Break A strategically timed break provided valuable opportunities for:\nNetworking with other participants and AWS technical experts Informal discussions about AI/ML use cases, challenges, and solutions Refreshments and casual conversations about workshop content and applications Sharing experiences and learning from peers\u0026rsquo; perspectives 10:45 AM – 12:00 PM | Generative AI with Amazon Bedrock This session explored the revolutionary capabilities of Amazon Bedrock for building next-generation generative AI applications.\nFoundation Models: Claude, Llama, Titan – Comparison \u0026amp; Selection Guide Understanding Foundation Models:\nThe session provided comprehensive insights into the foundation model landscape:\nModel Ecosystem Overview: Detailed examination of different foundation models available on Bedrock, including Claude (Anthropic), Llama (Meta), Titan (AWS), and other leading models Architectural Understanding: Understanding the underlying architectures and their unique strengths and characteristics Capability Comparison: Systematic comparison of model capabilities across diverse tasks including text generation, summarization, question answering, and code generation Model Selection Strategy:\nPractical guidance for choosing the right model:\nPerformance Characteristics: Comparing models on various benchmarks and real-world use cases to understand performance trade-offs Use Case Suitability: Understanding which models excel for specific applications (e.g., creative writing, technical documentation, code generation) Cost Analysis: Analyzing pricing models, token costs, and optimization strategies for different models Latency and Throughput: Comparing inference speed, scalability, and response time characteristics Best Practices for Model Selection:\nCriteria for selecting the optimal model based on specific business needs and requirements Balancing performance, cost, and latency requirements to find the right fit Strategies for experimenting with multiple models to determine the best choice When to use multiple models for different components of a single application Cost Optimization Strategies:\nUnderstanding pricing models and token-based costs for different foundation models Techniques for reducing inference costs through caching, batching, and efficient prompt design Caching strategies for frequently used prompts and responses Batch processing approaches for cost-effective scaling Prompt Engineering: Techniques, Chain-of-Thought Reasoning, Few-shot Learning Prompt Engineering Fundamentals:\nMastering the art of effective prompt design:\nCrafting Effective Prompts: Learning systematic approaches to design prompts that elicit desired outputs from language models Prompt Structure Impact: Understanding the critical relationship between prompt structure, clarity, and model response quality Improvement Techniques: Methods for enhancing prompt clarity, specificity, and effectiveness Common Pitfalls: Identifying and avoiding frequent mistakes in prompt design that lead to poor results Chain-of-Thought Reasoning:\nAdvanced reasoning techniques for complex problem-solving:\nStep-by-Step Guidance: Understanding how to guide models through structured, step-by-step reasoning processes Problem Decomposition: Techniques for breaking down complex problems into manageable, sequential steps Practical Examples: Real-world examples of chain-of-thought prompting for mathematical, logical, and analytical tasks Accuracy Improvement: How chain-of-thought reasoning significantly improves model accuracy on complex reasoning tasks Few-shot Learning:\nLeveraging examples to enhance model performance:\nExample-Based Learning: Techniques for providing strategic examples to improve model performance on specific tasks Example Influence: Understanding how few-shot examples shape and influence model behavior Best Practices: Guidelines for selecting, formatting, and ordering examples for maximum effectiveness When to Use: Determining when few-shot learning is more effective than fine-tuning for specific use cases Quality vs. Quantity: Balancing the number and quality of examples for optimal results Advanced Prompting Techniques:\nRole-based prompting for consistent, character-aligned outputs Template-based approaches for structured, predictable outputs Prompt chaining for complex, multi-step tasks Error handling and retry strategies for robust applications Retrieval-Augmented Generation (RAG): Architecture \u0026amp; Knowledge Base Integration RAG Architecture Overview:\nUnderstanding the RAG pattern and its transformative impact:\nRAG Fundamentals: Understanding how RAG elegantly combines information retrieval with generative capabilities Vector Database Role: The critical role of vector databases in RAG systems for semantic search and retrieval Accuracy Improvement: How RAG dramatically improves accuracy and reduces hallucinations compared to standalone generation Fine-tuning Comparison: Comparing RAG approaches with traditional fine-tuning methods Knowledge Base Integration:\nPractical implementation of knowledge bases:\nVector Database Integration: Learning to connect Bedrock with vector databases and knowledge bases Amazon OpenSearch: Utilizing OpenSearch as a powerful vector store for RAG applications with semantic search capabilities Amazon Kendra: Leveraging Kendra\u0026rsquo;s intelligent search and knowledge management capabilities Custom Knowledge Bases: Building custom knowledge bases using S3 and vector databases for domain-specific applications Data Ingestion: Best practices for ingesting, processing, and indexing documents efficiently Implementation Patterns:\nBest practices for building RAG applications that deliver accurate, context-aware responses Document chunking strategies for optimal retrieval performance Embedding generation, storage, and retrieval optimization Advanced retrieval strategies and ranking algorithms Response generation techniques that effectively incorporate retrieved context RAG Optimization:\nStrategies for improving retrieval accuracy and relevance Techniques for reducing latency in RAG systems Cost optimization approaches for RAG applications Monitoring and evaluation metrics for RAG performance Bedrock Agents: Multi-step Workflows and Tool Integrations Agent Architecture:\nUnderstanding intelligent agent capabilities:\nWorkflow Orchestration: Understanding how Bedrock Agents can orchestrate complex, multi-step workflows intelligently Task Decomposition: The role of agents in breaking down complex tasks into manageable steps Decision-Making: Agent decision-making and planning capabilities for autonomous operation Comparison with Simple Approaches: Understanding when agents provide value over simple prompt-based approaches Tool Integration:\nConnecting agents with external systems:\nExternal API Integration: Learning to connect agents with external APIs, databases, and AWS services Custom Tool Development: Building custom tools for agents to extend functionality Lambda Integration: Integrating with Lambda functions for custom business logic Database Connections: Connecting to various databases and data sources API Integration Patterns: Best practices for reliable API integration Workflow Design:\nPatterns for designing agent-based applications that handle complex user requests elegantly Multi-turn conversation handling and context management Error recovery and fallback strategies for robust applications State management in agent workflows User experience considerations for agent interactions Agent Capabilities:\nIntelligent task planning and execution Dynamic tool selection and usage Iterative refinement of responses based on feedback Handling ambiguous and incomplete user requests Guardrails: Safety and Content Filtering Content Safety:\nEnsuring safe AI applications:\nBedrock Guardrails: Understanding Bedrock Guardrails for comprehensive filtering of harmful or inappropriate content Pre-built Filters: Exploring pre-built safety filters and their capabilities Real-time Filtering: Real-time content filtering during inference to prevent harmful outputs Blocking Strategies: Effective blocking and filtering strategies for various content types Custom Policies:\nTailoring safety to business needs:\nCustom Configuration: Learning to configure custom content filters based on specific business requirements Policy Definition: Defining custom guardrail policies for industry-specific needs Compliance Requirements: Addressing industry-specific compliance requirements Balance Optimization: Balancing safety measures with application functionality Compliance and Governance:\nBest practices for ensuring AI applications meet regulatory and ethical standards Comprehensive audit trails and logging for compliance verification Data privacy considerations and GDPR compliance Ethical AI practices and responsible AI development Regulatory compliance frameworks (GDPR, CCPA, etc.) Implementation Best Practices:\nIntegrating guardrails seamlessly into applications Testing and validation strategies for guardrail effectiveness Monitoring guardrail performance and effectiveness Continuous improvement of safety measures based on real-world usage Live Demo: Building a Generative AI Chatbot using Bedrock The demonstration provided a complete, end-to-end walkthrough of building a production-ready chatbot application:\nSetting Up Bedrock Foundation Model:\nSelecting and configuring the appropriate foundation model for the use case Setting up model parameters and inference settings for optimal performance Understanding model capabilities, limitations, and best use cases Implementing RAG with Knowledge Base Integration:\nCreating a comprehensive knowledge base with relevant documents Setting up vector embeddings and efficient retrieval mechanisms Integrating RAG seamlessly into the chatbot workflow Testing and validating retrieval accuracy Configuring Bedrock Agents:\nSetting up intelligent agents for sophisticated multi-turn conversations Defining agent tools, capabilities, and decision-making logic Configuring agent behavior, personality, and response style Implementing conversation context management Adding Guardrails:\nImplementing comprehensive content safety filters Configuring custom guardrail policies for specific requirements Testing guardrail effectiveness with various inputs Balancing safety measures with user experience Deploying the Chatbot:\nDeployment architecture and scalability considerations Integration with frontend applications and user interfaces Comprehensive monitoring and logging setup Scaling strategies for production workloads Key Insight: The demo demonstrated that building sophisticated, production-ready AI applications is now more accessible than ever, with AWS providing the comprehensive tools and infrastructure needed to transform concepts into deployed solutions rapidly.\nKey Highlights Comprehensive ML Platform SageMaker\u0026rsquo;s End-to-End Solution:\nSageMaker delivers a complete, integrated solution for machine learning that covers every stage from initial data exploration to production deployment and continuous monitoring. This comprehensive platform eliminates the complexity of integrating multiple disparate tools, significantly reducing operational overhead and improving team productivity.\nKey Benefits:\nUnified interface that streamlines the entire ML lifecycle Integrated MLOps capabilities for reliable production deployment Scalable infrastructure that seamlessly handles everything from experimentation to production workloads Built-in best practices, security features, and compliance tools Generative AI Capabilities Amazon Bedrock\u0026rsquo;s Foundation Model Access:\nAmazon Bedrock provides flexible access to multiple state-of-the-art foundation models, enabling easy experimentation and informed selection of the optimal model for each specific use case. This flexibility is crucial for building applications that precisely meet unique business requirements.\nAdvantages:\nMultiple model options without vendor lock-in, providing flexibility and choice Easy experimentation and comparison across different models Cost-effective access to cutting-edge models without infrastructure investment Consistent API interface across different models, simplifying integration RAG Architecture Knowledge-Enhanced AI Applications:\nThe RAG pattern enables building sophisticated AI applications that can access and utilize specific, domain-relevant knowledge bases, dramatically improving accuracy and relevance. This architecture is essential for applications that need to provide accurate, up-to-date, and contextually relevant information.\nBenefits:\nAccess to specific, domain-specific knowledge without model retraining Significantly reduced hallucinations and improved factual accuracy No need for expensive and time-consuming model fine-tuning Easy to update knowledge bases as information evolves Production-Ready MLOps SageMaker\u0026rsquo;s Integrated MLOps:\nSageMaker\u0026rsquo;s comprehensive MLOps capabilities simplify the complex process of deploying and maintaining ML models in production environments. This is crucial for organizations that need reliable, scalable, and maintainable ML systems.\nFeatures:\nAutomated model versioning and comprehensive tracking Production monitoring with automated alerting Automated retraining pipelines for continuous improvement Model governance and compliance management Safety First Bedrock Guardrails:\nBedrock Guardrails ensure that generative AI applications are safe, compliant, and aligned with business values and ethical standards. This is essential for deploying AI applications in production environments where safety and trust are paramount.\nImportance:\nProtects users from harmful, inappropriate, or biased content Ensures regulatory compliance and legal protection Maintains brand reputation and user trust Builds confidence in AI-powered applications Key Learnings Technical Insights SageMaker Studio Productivity: SageMaker Studio provides a unified interface for the entire ML lifecycle, dramatically improving developer productivity. The ability to work within a single, integrated environment eliminates context switching, reduces setup time, and streamlines the development workflow from experimentation to production.\nFoundation Model Selection: Foundation model selection is crucial and highly dependent on specific use cases, performance requirements, and cost constraints. There\u0026rsquo;s no universal solution, and systematic experimentation is essential for identifying the optimal model for each application.\nPrompt Engineering Criticality: Prompt engineering is a critical skill that can dramatically improve model outputs without requiring expensive fine-tuning. Well-crafted prompts can transform mediocre results into exceptional performance, making this skill essential for AI developers.\nRAG Architecture Essentiality: RAG architecture is essential for building AI applications that need access to specific, up-to-date information. This pattern is rapidly becoming the standard for knowledge-intensive applications, providing accuracy and relevance that standalone models cannot match.\nBedrock Agents Sophistication: Bedrock Agents enable building sophisticated AI applications that can handle complex, multi-step workflows autonomously. This capability opens up possibilities for building truly intelligent applications that can reason, plan, and execute complex tasks.\nContent Safety Priority: Content safety must be considered from the very beginning when building generative AI applications. Guardrails should be integrated into the design architecture, not added as an afterthought, to ensure comprehensive protection.\nStrategic Insights Start with Use Cases: Always begin by identifying specific business problems and requirements before selecting AI/ML solutions. Technology should serve business objectives, not drive them. Understanding the problem deeply leads to better solution selection.\nFoundation Models are Powerful: Pre-trained foundation models can solve many problems effectively without custom training, significantly reducing time-to-market and development costs. Leveraging these models intelligently can accelerate innovation.\nRAG is Essential: For applications requiring specific knowledge or domain expertise, RAG architecture is the optimal approach. It provides the accuracy, relevance, and updatability needed for production applications without the complexity of fine-tuning.\nMLOps Matters: Proper MLOps practices are crucial for maintaining ML models in production. Without robust MLOps, models can degrade over time, become unreliable, and fail to deliver expected business value.\nSafety Cannot be Overlooked: Content filtering and safety measures must be integrated from the design phase. Addressing safety as an afterthought can lead to significant problems, compliance issues, and damage to brand reputation.\nContinuous Learning: The AI/ML landscape evolves at an unprecedented pace, requiring continuous learning and experimentation. Staying current with new models, techniques, and best practices is essential for long-term success in this field.\nApplication to My Work Based on the comprehensive insights gained from this workshop, I plan to apply the following strategies in my internship and future projects:\nImmediate Actions Experiment with SageMaker:\nSet up SageMaker Studio to explore ML model development for data analysis and predictive analytics projects Build end-to-end ML pipelines that demonstrate the complete workflow from data to deployment Experiment with different algorithms, hyperparameter tuning, and model optimization techniques Build RAG Applications:\nImplement RAG architecture using Bedrock and knowledge bases for practical applications Create internal documentation Q\u0026amp;A systems that can answer questions about company processes and policies Build domain-specific knowledge assistants that can provide accurate, context-aware information Prompt Engineering Practice:\nDevelop prompt engineering skills by creating templates and best practices for common use cases Build a library of effective prompts that can be reused across projects Practice chain-of-thought and few-shot learning techniques to improve model outputs MLOps Integration:\nApply SageMaker\u0026rsquo;s MLOps capabilities to automate model training and deployment pipelines Set up model monitoring and automated retraining pipelines for continuous improvement Implement CI/CD practices for ML models to ensure reliable, repeatable deployments Safety Implementation:\nIntegrate Bedrock Guardrails into any generative AI applications I develop Develop custom guardrail policies tailored to specific use cases and requirements Ensure compliance and safety considerations are addressed from the design phase Long-Term Strategies AI/ML Skill Development:\nContinuously learn about new foundation models and their evolving capabilities Stay updated with best practices in prompt engineering and RAG optimization Explore advanced patterns and techniques for building sophisticated AI applications Production Deployment:\nLearn to deploy ML models and GenAI applications to production environments Understand scaling strategies, cost optimization, and performance tuning Master monitoring, maintenance, and continuous improvement of AI systems Ethical AI Practices:\nDevelop expertise in AI safety, compliance, and ethical considerations Understand regulatory requirements for AI applications in different industries Contribute to building responsible, trustworthy AI systems Personal Experience This workshop provided an exceptional, hands-on introduction to AWS AI/ML services, masterfully combining theoretical knowledge with practical demonstrations and real-world application scenarios.\nLearning from Demonstrations SageMaker Studio Demo:\nThe SageMaker Studio demonstration was particularly impressive and eye-opening. It showcased how a unified platform can dramatically streamline the entire ML workflow, from initial data exploration through model deployment. The ability to work within a single, integrated environment eliminates the traditional friction of switching between multiple tools. The visual workflow builder and integrated MLOps capabilities demonstrated how modern ML platforms can significantly reduce the complexity of building production ML systems, making advanced ML accessible to a broader range of developers.\nRAG Architecture Understanding:\nLearning about RAG architecture was transformative. The practical examples clearly demonstrated how to build AI applications that can leverage specific knowledge bases effectively. The session showed how RAG can provide accurate, context-aware responses by intelligently combining information retrieval with generative capabilities. This understanding is crucial for building AI applications that need to provide accurate, domain-specific information, which is essential for most real-world business applications.\nBedrock Agents Demonstration:\nThe Bedrock Agents demonstration revealed the tremendous potential for building sophisticated AI applications that can handle complex, multi-step workflows autonomously. Seeing how agents can break down complex tasks, intelligently select and use tools, and orchestrate multi-step processes was inspiring. This capability opens up possibilities for building truly intelligent applications that can handle real-world complexity, moving beyond simple question-answering to sophisticated problem-solving.\nPrompt Engineering Focus:\nThe practical focus on prompt engineering provided immediately applicable skills for working with language models. Learning techniques like chain-of-thought reasoning and few-shot learning gave me concrete, actionable tools to improve model outputs significantly. The emphasis on prompt engineering as a critical skill highlighted that working effectively with AI models requires both technical knowledge and creative problem-solving abilities.\nGuardrails Understanding:\nUnderstanding Guardrails helped me appreciate the fundamental importance of safety and compliance in AI applications. The demonstration showed how guardrails can be integrated seamlessly while maintaining application functionality and user experience. This reinforced that building responsible AI is not just about technology but requires thoughtful design, careful implementation, and ongoing monitoring.\nKey Realizations Accessibility of AI/ML:\nThe workshop demonstrated that building AI/ML applications is now more accessible than ever before. AWS provides comprehensive tools and infrastructure needed to transform concepts into production systems without requiring deep expertise in every aspect of ML. This democratization of AI is empowering for developers at all levels, enabling innovation that was previously limited to specialized teams.\nImportance of Architecture:\nThe workshop reinforced that choosing the right architecture (RAG, agents, etc.) is crucial for building effective AI applications. Understanding when to use different patterns, how to combine them effectively, and how to optimize them for specific use cases is a key skill for AI developers. Architecture decisions have profound impacts on performance, cost, and maintainability.\nSafety as a Foundation:\nThe emphasis on guardrails and safety highlighted that responsible AI development requires thinking about safety from the very beginning of the design process. This is not just about compliance but about building trust, ensuring positive user experiences, and protecting brand reputation. Safety should be a foundational consideration, not an add-on.\nContinuous Evolution:\nThe workshop made it clear that the AI/ML landscape is evolving at an unprecedented pace. New models, techniques, and best practices emerge regularly, requiring continuous learning, adaptation, and experimentation. Staying current is not optional but essential for success in this rapidly changing field.\nImpact on My Career This workshop has significantly expanded my understanding of AI/ML capabilities and how to build production-ready applications effectively. The hands-on experience with SageMaker and Bedrock has given me practical, immediately applicable skills that I can use in real projects. The workshop has also inspired me to:\nExplore more advanced AI/ML techniques and patterns Build practical AI applications that solve real business problems Contribute to responsible AI development practices Stay current with the rapidly evolving AI/ML landscape Takeaways Strategic Principles Start with Use Cases: Always begin by identifying specific business problems and requirements before selecting AI/ML solutions. Technology should solve real problems and deliver measurable value, not be implemented for its own sake. Deep problem understanding leads to better solution selection and implementation.\nFoundation Models are Powerful: Pre-trained foundation models can solve many problems effectively without custom training, significantly accelerating development and reducing costs. Intelligently leveraging these models can transform development timelines and enable rapid innovation.\nRAG is Essential: For applications requiring specific knowledge or domain expertise, RAG architecture is the optimal approach. It provides the accuracy, relevance, and updatability needed for production applications without the complexity and cost of model fine-tuning.\nMLOps Matters: Proper MLOps practices are crucial for maintaining ML models in production. Without robust MLOps, models can degrade over time, become unreliable, and fail to deliver expected business value. MLOps is not optional but essential for production ML systems.\nSafety Cannot be Overlooked: Content filtering and safety measures must be integrated from the design phase. Addressing safety as an afterthought can lead to significant problems, compliance issues, and damage to brand reputation. Safety is a foundational requirement, not a nice-to-have.\nContinuous Learning: The AI/ML landscape evolves at an unprecedented pace, requiring continuous learning, experimentation, and adaptation. Staying current with new models, techniques, and best practices is essential for long-term success in this rapidly changing field.\nEvent Photos Add your event photos here\nThe AI/ML/GenAI on AWS Workshop was a comprehensive and practical introduction to building AI applications on AWS. The masterful combination of theoretical knowledge, hands-on demonstrations, and real-world examples provided a solid foundation for working with SageMaker and Bedrock. The insights gained from this workshop have equipped me with the knowledge and skills needed to build production-ready AI/ML applications and have inspired me to continue exploring the rapidly evolving, exciting world of artificial intelligence. This experience has fundamentally shaped my understanding of how AI can be applied practically to solve real-world problems.\n"
},
{
	"uri": "http://localhost:1313/aws_ojt_report-TranThiThuHa/4-eventparticipated/4.4-event4/",
	"title": "Event 4",
	"tags": [],
	"description": "",
	"content": "Event Report: DevOps on AWS Workshop Event Overview Event Name: DevOps on AWS Workshop\nDate \u0026amp; Time: Monday, November 17, 2025, 8:30 AM – 5:00 PM\nLocation: AWS Vietnam Office\nMy Role: Attendee\nEvent Purpose The DevOps on AWS Workshop was meticulously structured to deliver comprehensive knowledge and practical, hands-on experience with AWS DevOps services. The event strategically covered critical domains including:\nCI/CD Pipelines: Understanding continuous integration and continuous deployment workflows Infrastructure as Code: Exploring automated infrastructure management approaches Container Services: Delving into containerization and orchestration on AWS Monitoring \u0026amp; Observability: Mastering comprehensive system visibility and performance tracking The workshop\u0026rsquo;s primary objective was to help participants understand DevOps culture, principles, and best practices while exploring practical implementation of DevOps workflows on AWS infrastructure. The event emphasized that DevOps is not just about tools, but a cultural transformation that enables organizations to deliver software faster, more reliably, and with higher quality.\nAgenda Overview Morning Session (8:30 AM – 12:00 PM) 8:30 – 9:00 AM | Welcome \u0026amp; DevOps Mindset The workshop commenced with an engaging welcome session that established the foundation for understanding DevOps as both a culture and a practice:\nRecap of Previous Workshop:\nBrief review of AI/ML concepts from the previous workshop session Connecting AI/ML workflows with DevOps practices Understanding how DevOps enables AI/ML model deployment and operations DevOps Culture and Principles:\nThe session provided deep insights into the cultural transformation required for DevOps success:\nCultural Shift: Understanding the fundamental transformation from traditional IT silos to collaborative, cross-functional teams Core Principles: Collaboration: Breaking down barriers between development and operations teams Automation: Eliminating manual, error-prone processes through intelligent automation Continuous Improvement: Embracing a culture of learning, experimentation, and iterative enhancement Mindset Transformation: Moving from \u0026ldquo;throwing code over the wall\u0026rdquo; to shared ownership and responsibility Benefits and Key Metrics:\nThe session introduced critical metrics for measuring DevOps success:\nDORA Metrics (DevOps Research and Assessment):\nDeployment Frequency: Measuring how often teams successfully deploy code to production Lead Time for Changes: Tracking the time from code commit to production deployment Mean Time To Recovery (MTTR): Measuring how quickly teams can recover from failures and restore service Change Failure Rate: Tracking the percentage of deployments that result in failures requiring remediation Operational Performance Metrics:\nMTTR Analysis: Understanding how DevOps practices significantly reduce recovery times Deployment Frequency Tracking: Measuring improvements in deployment velocity Quality Metrics: Correlation between DevOps practices and reduced defect rates Discussion on DevOps Impact:\nHow DevOps practices dramatically improve software delivery speed and quality Operational performance improvements through automation and monitoring Real-world examples of organizations achieving significant improvements in DORA metrics The relationship between DevOps maturity and business outcomes 9:00 – 10:30 AM | AWS DevOps Services – CI/CD Pipeline This comprehensive session provided deep technical insights into building complete CI/CD pipelines on AWS.\nSource Control: AWS CodeCommit, Git Strategies AWS CodeCommit:\nThe session covered AWS\u0026rsquo;s fully managed source control service:\nFully Managed Service: Secure, scalable Git repositories without infrastructure management Security Features: Encryption at rest and in transit, IAM integration, and access control Integration: Seamless integration with other AWS services and third-party tools Scalability: Handling repositories of any size with high availability Git Strategies:\nComprehensive coverage of branching strategies for different team needs:\nGitFlow Workflow:\nFeature Branches: Isolated development for new features Develop Branch: Integration branch for completed features Release Branches: Preparing releases with bug fixes and stabilization Master Branch: Production-ready code Use Cases: Suitable for teams with scheduled releases and multiple parallel features Trunk-Based Development:\nMain Branch Focus: All development happens on or very close to the main branch Short-Lived Feature Branches: Features are merged quickly, typically within days Continuous Integration: Frequent commits and merges to main branch Use Cases: Ideal for teams practicing continuous deployment and rapid iteration Best Practices for Branching:\nChoosing the right strategy based on team size, release cadence, and project requirements Balancing between isolation and integration needs Managing conflicts and merge strategies Code review processes and quality gates Build \u0026amp; Test: CodeBuild Configuration, Testing Pipelines AWS CodeBuild:\nDeep dive into AWS\u0026rsquo;s fully managed build service:\nFully Managed Build Service: Compiles source code, runs tests, and produces deployment-ready packages Scalability: Automatically scales build capacity based on demand Cost Efficiency: Pay only for build compute time used Multi-Language Support: Supports various programming languages and build tools Build Configuration:\nBuildspec Files: YAML-based configuration files that define build commands and settings Environment Variables: Managing secrets, configuration, and build parameters Build Artifacts: Configuring output artifacts for deployment Custom Build Environments: Using custom Docker images for specialized build requirements Testing Pipelines:\nComprehensive testing strategies:\nUnit Tests: Fast, isolated tests for individual components Integration Tests: Testing interactions between components Automated Test Execution: Running tests automatically in CI/CD pipelines Test Reporting: Collecting and reporting test results Integration with Testing Frameworks: Connecting with popular testing tools (JUnit, pytest, Jest, etc.) Code Quality Tools: Integrating static analysis, linting, and code coverage tools Deployment: CodeDeploy with Blue/Green, Canary, and Rolling Updates AWS CodeDeploy:\nUnderstanding automated application deployment:\nAutomated Deployments: Deploying to EC2, Lambda, or on-premises servers Zero-Downtime Deployments: Advanced deployment strategies for high availability Rollback Capabilities: Automatic and manual rollback options Multi-Environment Support: Managing deployments across development, staging, and production Deployment Strategies:\nBlue/Green Deployment:\nZero-Downtime Approach: Running two identical production environments simultaneously Traffic Switching: Instant switching between environments Rollback: Immediate rollback by switching traffic back Use Cases: Critical applications requiring zero downtime Canary Deployment:\nGradual Rollout: Deploying to a small percentage of users first Monitoring: Observing metrics and user feedback before full deployment Risk Mitigation: Limiting impact of potential issues Use Cases: Applications with large user bases and high risk tolerance requirements Rolling Updates:\nIncremental Deployment: Deploying updates across instances incrementally Automatic Rollback: Rolling back automatically if health checks fail Resource Efficiency: Maintaining full capacity during deployment Use Cases: Stateless applications and services Choosing the Right Strategy:\nFactors to consider: application type, risk tolerance, and infrastructure requirements Cost implications of different strategies Operational complexity and team expertise Orchestration: CodePipeline Automation AWS CodePipeline:\nMastering the fully managed continuous delivery service:\nFully Managed Service: Automating release pipelines without infrastructure management Visual Pipeline: Visual representation of the entire software delivery process Integration Hub: Connecting various AWS services and third-party tools Event-Driven: Automated triggers based on code changes Pipeline Stages:\nSource Stage: Pulling code from source repositories (CodeCommit, GitHub, S3) Build Stage: Compiling and testing code using CodeBuild Test Stage: Running additional tests and quality checks Deploy Stage: Deploying to various environments using CodeDeploy Approval Stages: Manual approvals for production deployments Custom Actions: Integrating custom actions and third-party tools Integration and Automation:\nService Integration: Connecting CodeCommit, CodeBuild, CodeDeploy, and other AWS services Automated Triggers: Triggering pipelines on code commits, schedule, or manual execution Parallel Execution: Running multiple stages or actions in parallel for faster pipelines Pipeline Visualization: Monitoring pipeline execution and identifying bottlenecks Error Handling: Managing failures and retries in pipeline stages Demo: Full CI/CD Pipeline Walkthrough The demonstration provided a complete, end-to-end walkthrough of building a production-ready CI/CD pipeline:\nSetting Up CodeCommit Repository:\nCreating a new repository and configuring access Initializing the repository with sample application code Setting up branch protection and collaboration workflows Configuring CodeBuild:\nCreating build projects with buildspec configuration Setting up automated builds and tests Configuring build artifacts and environment variables Testing build processes Creating CodeDeploy Application:\nSetting up CodeDeploy application and deployment groups Configuring Blue/Green deployment strategy Setting up health checks and rollback conditions Testing deployment process Building CodePipeline:\nCreating pipeline with all stages Connecting source, build, and deploy stages Configuring triggers and approvals Setting up notifications and monitoring Testing the Pipeline:\nMaking code changes and observing automated pipeline execution Verifying automated build and test execution Observing automated deployment process Testing rollback capabilities Key Insight: The demo demonstrated that building a complete CI/CD pipeline is now more accessible than ever, with AWS providing all the necessary services integrated seamlessly.\n10:30 – 10:45 AM | Break A well-timed break provided opportunities for:\nNetworking with other participants and AWS experts Informal discussions about DevOps challenges and solutions Refreshments and casual conversations about workshop content 10:45 AM – 12:00 PM | Infrastructure as Code (IaC) This session explored the powerful capabilities of Infrastructure as Code for managing AWS infrastructure.\nAWS CloudFormation: Templates, Stacks, and Drift Detection CloudFormation Fundamentals:\nUnderstanding AWS\u0026rsquo;s native IaC service:\nTemplate-Based: JSON/YAML templates for defining AWS resources declaratively Stack Management: Collections of AWS resources managed as a single unit Idempotency: Safe to run multiple times with predictable results Resource Management: Creating, updating, and deleting resources automatically CloudFormation Templates:\nTemplate Structure: Understanding template sections (Parameters, Resources, Outputs, etc.) Intrinsic Functions: Using built-in functions for dynamic resource configuration Template Organization: Best practices for organizing and modularizing templates Parameterization: Making templates reusable across environments Nested Stacks: Breaking complex infrastructure into manageable components Stack Operations:\nStack Creation: Creating new infrastructure from templates Stack Updates: Updating infrastructure with change sets Stack Deletion: Safely removing infrastructure Drift Detection: Identifying changes made outside of CloudFormation Stack Policies: Controlling which resources can be updated or deleted Best Practices:\nTemplate organization and versioning Parameterization for reusability Using nested stacks for complex infrastructure Change set reviews before updates Stack naming conventions and tagging AWS CDK (Cloud Development Kit): Constructs, Reusable Patterns, and Language Support AWS CDK Overview:\nUnderstanding the modern approach to IaC:\nProgramming Languages: Define infrastructure using familiar languages (TypeScript, Python, Java, C#, Go, JavaScript) Higher-Level Abstractions: Building infrastructure using reusable constructs Type Safety: Compile-time checking and IDE support Testing: Writing unit tests for infrastructure code CDK Constructs:\nL1 Constructs: Low-level, direct mappings to CloudFormation resources L2 Constructs: Higher-level constructs with sensible defaults L3 Constructs: Patterns and best practices as reusable components Custom Constructs: Building reusable infrastructure components Reusable Patterns:\nPre-built Solutions: Common use cases like VPC, ECS clusters, serverless applications Pattern Library: AWS-maintained patterns for best practices Custom Patterns: Creating organization-specific patterns Pattern Composition: Combining patterns for complex architectures Language Support:\nTypeScript: Primary language with best support and examples Python: Popular for data science and ML teams Java/C#: Enterprise-friendly options Go: Performance-critical applications JavaScript: Web development teams Benefits Over CloudFormation:\nType safety and compile-time error detection IDE support with autocomplete and refactoring Easier testing with familiar testing frameworks Better code organization and reusability Familiar programming paradigms Demo: Deploying with CloudFormation and CDK The demonstration provided a side-by-side comparison of both IaC approaches:\nCloudFormation Deployment:\nCreating a VPC and EC2 instance using YAML template Demonstrating template structure and syntax Showing stack creation and resource provisioning Highlighting template-based approach CDK Deployment:\nSame infrastructure using TypeScript with CDK constructs Demonstrating code-based approach Showing type safety and IDE support Highlighting developer-friendly experience Comparison:\nDifferences in approach, syntax, and developer experience Maintainability and readability considerations When to choose each approach Hybrid approaches using both tools Discussion: Choosing between IaC Tools Decision Factors:\nTeam Expertise: Programming language familiarity and CloudFormation knowledge Project Complexity: Simple vs. complex infrastructure requirements Maintenance Requirements: Long-term maintainability and team preferences Integration Needs: Integration with existing tools and workflows When to Use CloudFormation:\nTeams comfortable with YAML/JSON Simple infrastructure requirements Need for direct CloudFormation resource control Compliance or organizational requirements When to Use CDK:\nDevelopment teams preferring code over templates Complex infrastructure with reusable patterns Need for type safety and testing Desire for better developer experience Hybrid Approaches:\nUsing CloudFormation for stable, well-understood resources Using CDK for new, complex infrastructure Combining both tools for different parts of infrastructure Migration strategies from CloudFormation to CDK Lunch Break (12:00 – 1:00 PM) Self-arranged lunch break with opportunities for informal networking and discussions.\nAfternoon Session (1:00 – 5:00 PM) 1:00 – 2:30 PM | Container Services on AWS This session explored containerization and container orchestration on AWS.\nDocker Fundamentals: Microservices and Containerization Containerization Concepts:\nUnderstanding the fundamentals:\nContainers: Lightweight, portable units that package applications and dependencies Images: Immutable templates for creating containers Containerization Benefits: Portability, consistency, and resource efficiency Isolation: Process and filesystem isolation between containers Microservices Architecture:\nBreaking Monoliths: Decomposing monolithic applications into smaller, independent services Service Independence: Each service can be developed, deployed, and scaled independently Communication: Service-to-service communication patterns Benefits: Faster development, easier scaling, and technology diversity Docker Basics:\nDockerfile: Instructions for building container images Image Building: Creating images from Dockerfiles Container Lifecycle: Creating, running, stopping, and removing containers Docker Compose: Managing multi-container applications locally Benefits:\nPortability: Run anywhere Docker is supported Consistency: Same behavior across development, testing, and production Resource Efficiency: Better resource utilization compared to VMs Rapid Deployment: Fast container startup and deployment Amazon ECR: Image Storage, Scanning, Lifecycle Policies Amazon ECR:\nUnderstanding AWS\u0026rsquo;s fully managed Docker container registry:\nFully Managed Service: Secure, scalable storage for Docker images Integration: Seamless integration with ECS, EKS, and other AWS services Security: Encryption at rest and in transit, IAM-based access control Scalability: Handling images of any size with high availability Image Storage:\nRepositories: Organizing images in repositories Image Tagging: Versioning and tagging strategies Image Pull/Push: Efficient image transfer Multi-Region: Replicating images across regions Image Scanning:\nAutomated Vulnerability Scanning: Identifying security vulnerabilities in images Scanning Reports: Detailed reports on vulnerabilities and remediation Integration: Automatic scanning on image push Compliance: Meeting security and compliance requirements Lifecycle Policies:\nAutomated Cleanup: Automatically removing old or unused images Retention Policies: Configuring image retention based on age or count Cost Optimization: Reducing storage costs by removing unused images Policy Configuration: Defining rules for image lifecycle management Amazon ECS \u0026amp; EKS: Deployment Strategies, Scaling, and Orchestration Amazon ECS:\nDeep dive into AWS\u0026rsquo;s fully managed container orchestration:\nFully Managed Service: Container orchestration without managing control plane Task Definitions: Container specifications, resource requirements, and networking Services: Long-running tasks with load balancing and auto-scaling Clusters: Grouping of container instances ECS Deployment Strategies:\nRolling Updates: Incremental updates with automatic rollback Blue/Green Deployments: Zero-downtime deployments using ALB Canary Deployments: Gradual rollout with monitoring Task Placement: Strategies for placing tasks on instances ECS Scaling:\nAuto-Scaling: Scaling based on CPU, memory, or custom metrics Service Auto-Scaling: Scaling services based on demand Cluster Auto-Scaling: Automatically adding/removing instances Scaling Policies: Configuring scaling behavior and thresholds Amazon EKS:\nUnderstanding managed Kubernetes service:\nManaged Control Plane: AWS manages Kubernetes control plane Kubernetes Compatibility: Standard Kubernetes API and tools Node Groups: Managing worker nodes Add-ons: Pre-configured Kubernetes add-ons Kubernetes Concepts:\nPods: Smallest deployable units in Kubernetes Services: Network abstraction for accessing pods Deployments: Managing pod replicas and updates Namespaces: Organizing resources within clusters EKS Deployment Strategies:\nRolling Updates: Standard Kubernetes rolling updates Canary Deployments: Using Istio or AWS App Mesh Blue/Green: Using multiple deployments and services Helm Charts: Package management for Kubernetes applications EKS Scaling:\nCluster Autoscaler: Automatically scaling node groups Horizontal Pod Autoscaler: Scaling pods based on metrics Vertical Pod Autoscaler: Adjusting pod resource requests Custom Metrics: Scaling based on application-specific metrics AWS App Runner: Simplified Container Deployment App Runner:\nUnderstanding the fully managed service for containerized applications:\nFully Managed: Building and running applications without infrastructure management Simplified Deployment: Deploy from source code or container image Auto-Scaling: Automatic scaling based on traffic Built-in CI/CD: Automatic builds and deployments Use Cases:\nWeb Applications: Simple web apps and APIs Microservices: Individual microservices Rapid Prototyping: Quick deployment for testing Low-Operational-Overhead: Applications requiring minimal operations Comparison: App Runner vs ECS vs EKS:\nComplexity: App Runner (lowest) → ECS → EKS (highest) Control: App Runner (least) → ECS → EKS (most) Use Cases: Choosing based on requirements and team expertise Migration Path: Moving between services as needs evolve Demo \u0026amp; Case Study: Microservices Deployment Comparison The demonstration provided a practical comparison of different container deployment options:\nDeploying with App Runner:\nSimple web application deployment Minimal configuration required Automatic scaling and management Low operational overhead Deploying with ECS Fargate:\nSame application with more control Task definition and service configuration Custom networking and scaling Moderate operational overhead Comparison Analysis:\nSetup complexity and time Cost implications Operational overhead Flexibility and control Scaling capabilities Case Study:\nChoosing the right service for different scenarios Startup vs. enterprise requirements Team expertise considerations Migration strategies 2:30 – 2:45 PM | Break Networking and refreshments.\n2:45 – 4:00 PM | Monitoring \u0026amp; Observability This session explored comprehensive monitoring and observability on AWS.\nCloudWatch: Metrics, Logs, Alarms, and Dashboards CloudWatch Metrics:\nUnderstanding metric collection and tracking:\nMetric Collection: Collecting metrics from AWS services and custom applications Custom Metrics: Publishing application-specific metrics Metric Namespaces: Organizing metrics logically Metric Dimensions: Filtering and aggregating metrics CloudWatch Logs:\nCentralized logging capabilities:\nLog Groups: Organizing logs by application or service Log Streams: Individual log sources within groups Log Retention: Configuring retention policies Log Insights: Querying and analyzing logs CloudWatch Alarms:\nAutomated monitoring and alerting:\nAlarm Configuration: Setting thresholds and evaluation periods Alarm Actions: Triggering actions based on alarm state Composite Alarms: Combining multiple alarms Anomaly Detection: Using machine learning for anomaly detection CloudWatch Dashboards:\nVisualization and monitoring:\nCustomizable Dashboards: Creating dashboards for different audiences Widget Types: Various visualization options Dashboard Sharing: Sharing dashboards across teams Real-time Monitoring: Real-time metric visualization Best Practices:\nMetric naming conventions Log retention and archival Alarm configuration and thresholds Dashboard design for different audiences AWS X-Ray: Distributed Tracing and Performance Insights AWS X-Ray:\nUnderstanding distributed tracing:\nService for Analysis: Analyzing and debugging distributed applications End-to-End Tracing: Tracing requests across microservices Service Map: Visual representation of application architecture Performance Insights: Identifying bottlenecks and issues Distributed Tracing:\nRequest Tracing: Following requests through entire system Trace Segments: Understanding individual service contributions Trace Annotations: Adding custom metadata to traces Trace Filtering: Finding specific traces based on criteria Service Map:\nVisual Architecture: Understanding application structure Dependencies: Identifying service dependencies Health Indicators: Visual health status of services Performance Metrics: Service-level performance data Performance Insights:\nBottleneck Identification: Finding performance issues Latency Analysis: Understanding where time is spent Error Analysis: Identifying error patterns Optimization Opportunities: Finding areas for improvement Integration:\nX-Ray SDK: Integrating with applications AWS Service Integration: Automatic tracing for AWS services Third-Party Tools: Integration with monitoring tools Sampling: Configuring trace sampling for cost optimization Demo: Full-Stack Observability Setup The demonstration showed a complete observability setup:\nCloudWatch Setup:\nConfiguring metrics and logs for an application Setting up log groups and streams Publishing custom metrics Dashboard Creation:\nCreating dashboards for developers Creating dashboards for operations Creating executive dashboards Configuring real-time updates Alarm Configuration:\nSetting up critical alarms Configuring alarm actions Testing alarm functionality Setting up notification channels X-Ray Integration:\nEnabling X-Ray tracing Viewing service maps Analyzing traces Identifying performance issues Best Practices:\nAlerting strategy and avoiding alert fatigue Dashboard design principles On-call processes and escalation SLO/SLI definition and tracking Best Practices: Alerting, Dashboards, and On-Call Processes Alerting Strategy:\nMeaningful Alerts: Setting up alerts that require action Alert Fatigue: Avoiding too many alerts that get ignored Alert Prioritization: Categorizing alerts by severity Alert Routing: Routing alerts to appropriate teams Dashboard Design:\nAudience-Specific: Creating dashboards for different audiences Developer Dashboards: Technical metrics and logs Operations Dashboards: System health and performance Management Dashboards: Business metrics and KPIs On-Call Processes:\nIncident Response: Procedures for handling incidents Escalation Paths: Clear escalation procedures Runbooks: Documented procedures for common issues Communication: Channels for incident communication SLO/SLI:\nService Level Objectives: Defining reliability targets Service Level Indicators: Measuring actual performance Error Budgets: Managing reliability vs. feature velocity Reporting: Regular SLO/SLI reporting 4:00 – 4:45 PM | DevOps Best Practices \u0026amp; Case Studies This session explored real-world DevOps practices and transformations.\nDeployment Strategies: Feature Flags, A/B Testing Feature Flags:\nGradual Rollouts: Releasing features to subsets of users Canary Releases: Testing features with small user groups Instant Rollbacks: Quickly disabling features if issues arise Tools: AWS AppConfig, LaunchDarkly integration A/B Testing:\nVersion Comparison: Comparing different versions User Experience Optimization: Optimizing based on user behavior Statistical Significance: Ensuring valid test results Tools and Integration: A/B testing tools and frameworks Best Practices:\nFeature flag management strategies Testing strategies and validation Rollout planning and monitoring Risk mitigation approaches Automated Testing and CI/CD Integration Testing Pyramid:\nUnit Tests: Fast, isolated component tests Integration Tests: Testing component interactions End-to-End Tests: Full system testing Test Distribution: Balancing test types Test Automation:\nCI/CD Integration: Running tests automatically Test Execution: Parallel and sequential test execution Test Reporting: Collecting and reporting results Test Maintenance: Keeping tests up to date Quality Gates:\nBlocking Deployments: Preventing deployments with failing tests Quality Thresholds: Defining acceptable quality levels Test Coverage: Measuring and improving coverage Continuous Improvement: Iteratively improving test quality Incident Management and Postmortems Incident Response:\nDetection: Identifying incidents quickly Response: Coordinated response procedures Recovery: Restoring service functionality Communication: Keeping stakeholders informed Postmortems:\nLearning from Incidents: Documenting what happened Root Cause Analysis: Identifying underlying causes Action Items: Defining improvements Knowledge Sharing: Sharing learnings across teams Blameless Culture:\nFocus on Systems: Improving systems rather than blaming individuals Learning Mindset: Treating incidents as learning opportunities Continuous Improvement: Using incidents to drive improvements Psychological Safety: Creating safe environment for reporting Case Studies: Startups and Enterprise DevOps Transformations Startup Case Study:\nRapid Scaling: Scaling quickly with DevOps practices Cost Optimization: Managing costs while scaling Team Growth: Adapting practices as team grows Lessons Learned: Key takeaways from startup journey Enterprise Case Study:\nLarge-Scale Migration: Migrating to DevOps at scale Cultural Transformation: Changing organizational culture Tool Adoption: Selecting and implementing tools Change Management: Managing organizational change Lessons Learned:\nCommon Challenges: Typical obstacles and solutions Success Factors: What makes transformations successful ROI Measurement: Measuring DevOps impact Continuous Evolution: DevOps as ongoing journey 4:45 – 5:00 PM | Q\u0026amp;A \u0026amp; Wrap-up DevOps Career Pathways:\nCareer Progression: Paths in DevOps engineering Required Skills: Technical and soft skills needed Growth Opportunities: Career advancement options Industry Trends: Current and future trends AWS Certification Roadmap:\nAWS Certified DevOps Engineer – Professional: Advanced DevOps certification AWS Certified Solutions Architect: Architecture-focused certification AWS Certified SysOps Administrator: Operations-focused certification Certification Strategy: Planning certification journey Next Steps:\nLearning Resources: Recommended resources for continued learning Practice Opportunities: Hands-on practice recommendations Community Engagement: Joining DevOps communities Continuous Learning: Staying current with DevOps practices Closing Remarks:\nKey Takeaways: Summary of critical learnings Action Items: Immediate next steps for participants Workshop Impact: Expected impact on participants\u0026rsquo; work Future Opportunities: Continuing the DevOps journey Key Highlights CI/CD Pipeline Excellence AWS CodePipeline Complete Solution:\nAWS CodePipeline provides a comprehensive solution for automating software delivery from source code to production deployment. The integration of CodeCommit, CodeBuild, and CodeDeploy creates a seamless workflow that significantly reduces manual intervention and deployment errors.\nKey Benefits:\nComplete automation of software delivery pipeline Integration with multiple AWS services and third-party tools Visual pipeline representation for easy monitoring Support for multiple deployment strategies Infrastructure as Code Power CloudFormation and CDK:\nBoth CloudFormation and CDK offer powerful ways to manage infrastructure, each with distinct advantages. CloudFormation provides template-based declarative infrastructure, while CDK offers a code-based approach with better developer experience, type safety, and testing capabilities.\nAdvantages:\nVersion control for infrastructure Repeatable and consistent deployments Faster infrastructure changes Better collaboration and code review Container Services Flexibility Multiple Container Options:\nAWS offers multiple container options (ECS, EKS, App Runner) for different use cases and complexity levels. This flexibility allows organizations to choose the right service based on their specific requirements, team expertise, and operational capabilities.\nBenefits:\nChoose based on complexity and control needs Migration paths as requirements evolve Cost optimization opportunities Integration with AWS ecosystem Comprehensive Observability CloudWatch and X-Ray:\nCloudWatch and X-Ray together provide comprehensive monitoring and tracing capabilities that are essential for maintaining reliable systems. This combination enables teams to understand system behavior, identify issues quickly, and optimize performance.\nCapabilities:\nMetrics, logs, and alarms Distributed tracing Service maps and performance insights Custom dashboards and alerting DevOps Culture Foundation Cultural Transformation:\nDevOps success requires cultural change, not just tools and technology. The workshop emphasized that tools are enablers, but the cultural shift toward collaboration, automation, and continuous improvement is the foundation of DevOps success.\nKey Elements:\nCollaboration between teams Automation mindset Continuous improvement culture Shared ownership and responsibility Essential Best Practices Modern DevOps Practices:\nFeature flags, automated testing, and incident management are essential for modern DevOps. These practices enable teams to deploy with confidence, maintain quality, and respond to issues effectively.\nCritical Practices:\nFeature flags for gradual rollouts Comprehensive automated testing Effective incident management Continuous learning and improvement Key Learnings Strategic Insights DevOps is Culture First: Tools are important, but cultural transformation is the foundation of DevOps success. Organizations must focus on breaking down silos, fostering collaboration, and embracing automation and continuous improvement.\nCI/CD Automation Impact: Automating the entire software delivery pipeline significantly improves speed, reliability, and quality. The reduction in manual processes leads to fewer errors and faster time-to-market.\nIaC Benefits: Infrastructure as Code enables version control, repeatability, and faster infrastructure changes. This approach transforms infrastructure management from ad-hoc to systematic and predictable.\nContainer Strategy: Choosing the right container service depends on complexity, team expertise, and operational requirements. There\u0026rsquo;s no one-size-fits-all solution, and organizations should choose based on their specific needs.\nObservability Criticality: Comprehensive monitoring and tracing are essential for maintaining reliable systems. Without proper observability, teams operate blind and cannot effectively respond to issues or optimize performance.\nContinuous Improvement: DevOps is about continuous learning and improvement, not a one-time implementation. Organizations must embrace a culture of experimentation, learning from failures, and iterative enhancement.\nTechnical Insights Pipeline Design: Well-designed CI/CD pipelines significantly reduce deployment time and errors. The key is to automate everything that can be automated while maintaining quality gates.\nIaC Tool Selection: The choice between CloudFormation and CDK depends on team preferences, project complexity, and long-term maintainability. Both tools are powerful, and hybrid approaches are often effective.\nContainer Service Selection: Understanding the trade-offs between App Runner, ECS, and EKS is crucial for making the right choice. Each service has its place, and requirements may evolve over time.\nMonitoring Strategy: Effective monitoring requires a balance between comprehensive coverage and avoiding alert fatigue. Dashboards should be designed for specific audiences and use cases.\nTesting Integration: Automated testing integrated into CI/CD pipelines is essential for maintaining quality. The testing pyramid approach balances speed and coverage.\nIncident Management: Effective incident management requires preparation, clear procedures, and a blameless culture focused on learning and improvement.\nApplication to My Work Based on the comprehensive insights gained from this workshop, I plan to apply the following strategies in my internship and future projects:\nImmediate Actions Implement CI/CD:\nSet up CodePipeline for automated deployments in current projects Configure CodeBuild for automated builds and tests Implement CodeDeploy with appropriate deployment strategies Establish automated testing in the pipeline Adopt IaC:\nStart using CloudFormation or CDK for infrastructure management Convert existing manual infrastructure to IaC Establish infrastructure version control and review processes Create reusable infrastructure patterns Container Migration:\nEvaluate containerization opportunities for existing applications Start with simple applications using App Runner Gradually move to ECS or EKS as needed Implement container best practices Improve Monitoring:\nEnhance CloudWatch dashboards and alarms for better visibility Implement X-Ray tracing for distributed applications Establish meaningful alerting strategies Create dashboards for different audiences Practice DevOps:\nApply DevOps principles and practices in daily work Focus on automation and continuous improvement Foster collaboration with team members Embrace a culture of learning and experimentation Incident Management:\nEstablish incident response procedures Implement postmortem practices Create runbooks for common issues Foster a blameless culture Long-Term Strategies DevOps Maturity:\nContinuously improve DevOps practices Measure and track DORA metrics Implement advanced deployment strategies Adopt feature flags and A/B testing Skill Development:\nDeepen expertise in CI/CD tools and practices Master Infrastructure as Code Learn container orchestration in depth Develop monitoring and observability expertise Certification:\nPursue AWS Certified DevOps Engineer certification Consider Solutions Architect and SysOps certifications Stay current with AWS services and best practices Contribute to DevOps communities Personal Experience This full-day DevOps workshop was comprehensive, highly practical, and transformative. The combination of theoretical knowledge, hands-on demonstrations, and real-world case studies provided a solid foundation for understanding and implementing DevOps practices on AWS.\nLearning from Demonstrations CI/CD Pipeline Demo:\nThe CI/CD pipeline demonstration was particularly valuable, showing how to automate the entire software delivery process from code commit to production deployment. Seeing the complete workflow in action made the benefits of automation tangible and clear. The demonstration highlighted how AWS services integrate seamlessly to create a powerful automation platform.\nIaC Tools Comparison:\nLearning about different IaC tools helped me understand when to use CloudFormation versus CDK. The side-by-side comparison demonstrated the trade-offs between template-based and code-based approaches. This understanding is crucial for making informed decisions about infrastructure management strategies.\nContainer Services Comparison:\nThe container services comparison provided clear guidance on choosing the right service for different scenarios. The practical demonstrations showed the differences in setup complexity, operational overhead, and flexibility. This knowledge is essential for making the right architectural decisions.\nObservability Setup:\nThe observability session emphasized the importance of monitoring and tracing for maintaining reliable systems. The demonstration showed how CloudWatch and X-Ray work together to provide comprehensive visibility. Understanding how to set up effective monitoring is crucial for production systems.\nKey Realizations Culture is Foundation:\nThe workshop reinforced that DevOps is fundamentally about culture, not just tools. Tools enable DevOps practices, but the cultural transformation toward collaboration, automation, and continuous improvement is what drives success. This understanding will guide my approach to DevOps adoption.\nAutomation is Key:\nThe demonstrations clearly showed how automation transforms software delivery. Automating repetitive, error-prone processes not only improves speed but also significantly reduces errors and increases reliability. This principle will guide my approach to all development and operations work.\nObservability is Essential:\nThe observability session made it clear that comprehensive monitoring and tracing are not optional but essential for production systems. Without proper observability, teams cannot effectively maintain, troubleshoot, or optimize systems. This understanding will influence all my infrastructure and application design decisions.\nContinuous Learning:\nThe workshop emphasized that DevOps is a journey of continuous learning and improvement. The field evolves rapidly, and staying current requires ongoing learning and experimentation. This mindset will guide my professional development.\nImpact on My Career This workshop has significantly expanded my understanding of DevOps practices and how to implement them on AWS. The hands-on experience with AWS DevOps services has given me practical skills that I can immediately apply to projects. The workshop has also inspired me to:\nPursue DevOps engineering as a career path Continuously improve my DevOps skills and knowledge Contribute to DevOps transformations in organizations Stay current with evolving DevOps practices and tools Takeaways Strategic Principles Start Small: Begin with basic CI/CD automation and gradually expand DevOps practices. Trying to implement everything at once often leads to failure. Incremental adoption with continuous improvement is the path to success.\nCulture Matters: DevOps success requires team collaboration and cultural change. Tools alone are insufficient. Organizations must invest in cultural transformation, breaking down silos and fostering collaboration.\nChoose the Right Tools: Select tools based on team expertise and project requirements. There\u0026rsquo;s no universal best tool; the best tool is the one that fits the team and project context.\nMonitor Everything: Comprehensive observability is essential for reliable systems. Teams must implement comprehensive monitoring, logging, and tracing to understand system behavior and respond to issues effectively.\nLearn Continuously: DevOps practices evolve rapidly, requiring continuous learning. Staying current with new tools, techniques, and best practices is essential for long-term success in DevOps.\nMeasure Success: Use DORA metrics to track DevOps improvements and ROI. Measuring and tracking metrics provides visibility into progress and helps justify continued investment in DevOps practices.\nEvent Photos Add your event photos here\nThe DevOps on AWS Workshop was a comprehensive and transformative experience that provided deep insights into DevOps culture, practices, and AWS tools. The combination of theoretical knowledge, practical demonstrations, and real-world case studies equipped me with the knowledge and skills needed to implement DevOps practices effectively. The workshop reinforced that DevOps is a cultural transformation enabled by tools, not just a set of tools. The insights gained from this workshop will guide my approach to software development, infrastructure management, and team collaboration throughout my career. This experience has fundamentally shaped my understanding of how to build, deploy, and maintain software systems in a modern, efficient, and reliable manner.\n"
},
{
	"uri": "http://localhost:1313/aws_ojt_report-TranThiThuHa/4-eventparticipated/4.5-event5/",
	"title": "Event 5",
	"tags": [],
	"description": "",
	"content": "Event Report: AWS Well-Architected Security Pillar Workshop Event Overview Event Name: AWS Well-Architected Security Pillar Workshop\nDate \u0026amp; Time: Saturday, November 29, 2025, 8:30 AM – 12:00 PM\nLocation: AWS Vietnam Office\nMy Role: Attendee\nEvent Purpose This morning workshop provided a comprehensive, in-depth exploration of the AWS Well-Architected Security Pillar, covering all five security domains: Identity \u0026amp; Access Management, Detection, Infrastructure Protection, Data Protection, and Incident Response.\nThe workshop was designed to equip participants with practical knowledge for implementing security best practices in AWS environments, featuring real-world examples from Vietnamese enterprises. The session emphasized that security is not a one-time implementation but a continuous journey requiring ongoing vigilance and improvement.\nAgenda Overview 8:30 – 8:50 AM | Opening \u0026amp; Security Foundation Security Pillar in Well-Architected Framework:\nUnderstanding the role of Security Pillar within the Well-Architected Framework Core principles: Least Privilege, Zero Trust, Defense in Depth AWS Shared Responsibility Model and security boundaries Top cloud security threats in Vietnam\u0026rsquo;s context Key Concepts:\nLeast Privilege: Granting only the minimum permissions necessary Zero Trust: Never trust, always verify approach Defense in Depth: Multiple layers of security controls Shared Responsibility: Understanding AWS vs. customer security responsibilities 8:50 – 9:30 AM | Pillar 1 – Identity \u0026amp; Access Management Modern IAM Architecture:\nIAM Fundamentals:\nUsers, Roles, and Policies – avoiding long-term credentials Best practices for credential management Policy structure and evaluation logic IAM Identity Center:\nSingle Sign-On (SSO) capabilities Permission sets for centralized access management Multi-account access management Advanced IAM:\nService Control Policies (SCP) for multi-account environments Permission Boundaries for fine-grained access control Multi-Factor Authentication (MFA) implementation Credential rotation strategies IAM Access Analyzer for policy validation Mini Demo: Validating IAM policies and simulating access scenarios\n9:30 – 9:55 AM | Pillar 2 – Detection Detection \u0026amp; Continuous Monitoring:\nAWS Security Services:\nCloudTrail: Organization-level logging and audit trails GuardDuty: Intelligent threat detection service Security Hub: Centralized security findings and compliance Comprehensive Logging:\nVPC Flow Logs for network traffic analysis Application Load Balancer (ALB) access logs S3 access logs and bucket logging Log aggregation and analysis strategies Alerting \u0026amp; Automation:\nEventBridge for event-driven security automation Automated response to security events Integration with notification systems Detection-as-Code:\nInfrastructure as Code for detection rules Version-controlled security configurations Automated deployment of detection mechanisms 9:55 – 10:10 AM | Break Networking and refreshments.\n10:10 – 10:40 AM | Pillar 3 – Infrastructure Protection Network \u0026amp; Workload Security:\nVPC Security:\nVPC segmentation strategies Private vs. public subnet placement Network isolation and segmentation best practices Network Security Controls:\nSecurity Groups: Stateful firewall rules for EC2 instances Network ACLs: Stateless subnet-level filtering When to use Security Groups vs. NACLs Application patterns and use cases Application Protection:\nAWS WAF: Web application firewall for application protection AWS Shield: DDoS protection service Network Firewall: Managed network firewall service Integration strategies for comprehensive protection Workload Security:\nEC2 security best practices ECS security fundamentals EKS security basics Container security considerations 10:40 – 11:10 AM | Pillar 4 – Data Protection Encryption, Keys \u0026amp; Secrets:\nAWS KMS (Key Management Service):\nKey policies and access control Key grants for fine-grained permissions Key rotation strategies and automation Multi-region key management Encryption Implementation:\nEncryption at Rest: S3, EBS, RDS, DynamoDB encryption Encryption in Transit: TLS/SSL implementation Encryption best practices for different services Performance considerations Secrets Management:\nAWS Secrets Manager: Secure secret storage and rotation Systems Manager Parameter Store: Configuration and secrets management Rotation patterns and automation Integration with applications Data Classification \u0026amp; Access:\nData classification strategies Access guardrails based on data sensitivity Compliance and regulatory considerations 11:10 – 11:40 AM | Pillar 5 – Incident Response IR Playbook \u0026amp; Automation:\nIncident Response Lifecycle:\nUnderstanding the IR lifecycle according to AWS framework Preparation, detection, response, and recovery phases Continuous improvement and lessons learned Incident Response Playbooks:\nCompromised IAM Key:\nDetection and immediate response steps Key rotation and access revocation Investigation and remediation procedures S3 Public Exposure:\nIdentifying public bucket exposure Immediate remediation steps Access review and policy updates Prevention strategies EC2 Malware Detection:\nMalware detection and identification Instance isolation procedures Evidence collection and analysis Remediation and recovery Automation:\nAuto-response with Lambda functions Step Functions for orchestrated response Automated isolation and containment Evidence collection automation 11:40 – 12:00 PM | Wrap-up \u0026amp; Q\u0026amp;A Summary:\nRecap of all five security pillars Interconnections between pillars Comprehensive security architecture approach Common Pitfalls:\nSecurity mistakes common in Vietnamese enterprises Lessons learned from real-world scenarios Best practices to avoid common issues Learning Roadmap:\nAWS Security Specialty certification path Solutions Architect Professional security focus Continuous learning resources Community engagement opportunities Key Highlights Comprehensive Security Framework The AWS Well-Architected Security Pillar provides a complete framework covering all aspects of cloud security. The five pillars work together to create a defense-in-depth strategy that protects against various threats.\nPractical Implementation The workshop focused on practical, actionable guidance rather than theoretical concepts. Real-world examples from Vietnamese enterprises made the content immediately applicable.\nAutomation Focus Throughout all pillars, automation was emphasized as essential for maintaining security at scale. Manual security processes are error-prone and don\u0026rsquo;t scale effectively.\nContinuous Improvement Security is presented as a continuous journey, not a destination. Regular reviews, updates, and improvements are essential for maintaining effective security posture.\nKey Learnings Strategic Insights Least Privilege is Foundation: Always start with minimum permissions and expand only when necessary. This principle applies to all access control decisions.\nZero Trust Architecture: Never assume implicit trust, even within internal networks. Every access request should be verified and authorized.\nDefense in Depth: Security requires multiple layers of controls. No single security measure is sufficient on its own.\nAutomated Detection: Detection capabilities must be automated and continuous. Manual monitoring cannot keep pace with modern threats.\nIncident Response Readiness: Incident response playbooks must be documented, tested regularly, and updated based on lessons learned.\nTechnical Insights IAM Best Practices:\nAvoid long-term credentials where possible Use roles instead of users for applications Implement MFA for all privileged access Regularly review and rotate credentials Detection Strategy:\nImplement comprehensive logging at all layers Use GuardDuty and Security Hub for centralized detection Automate alerting and response mechanisms Practice Detection-as-Code for consistency Infrastructure Protection:\nProper VPC segmentation is critical Understand when to use Security Groups vs. NACLs Implement WAF and Shield for application protection Secure workloads at every layer Data Protection:\nEncrypt data at rest and in transit Use KMS for key management Implement secrets rotation Classify data and apply appropriate controls Incident Response:\nPrepare playbooks for common scenarios Automate response where possible Document and test procedures regularly Learn from each incident Application to My Work Based on the workshop insights, I plan to apply the following in my internship and future projects:\nImmediate Actions Implement IAM Access Analyzer:\nUse Access Analyzer to validate IAM policies in current projects Identify and remediate overly permissive policies Establish regular access reviews Set Up Security Monitoring:\nEnable GuardDuty for threat detection Configure Security Hub for centralized findings Set up CloudTrail organization-level logging Create Incident Response Playbooks:\nDocument playbooks for common security scenarios Test playbooks in practice exercises Establish incident response procedures Review Security Groups:\nAudit Security Group rules using least privilege principle Remove unnecessary open ports Document security group purposes Enable Encryption:\nEnable encryption for all data stores (S3, RDS, DynamoDB) Implement encryption in transit for all connections Use KMS for key management Long-Term Strategies Security Maturity:\nContinuously improve security posture Regular security reviews and audits Implement security automation Skill Development:\nPursue AWS Security Specialty certification Deepen knowledge in each security pillar Stay current with security best practices Security Culture:\nPromote security awareness Integrate security into development workflows Foster a security-first mindset Personal Experience This workshop was extremely valuable for gaining comprehensive understanding of AWS security:\nLearning from Practical Demos Real-World Examples: The practical demos helped make abstract security concepts concrete and understandable. Seeing security configurations in action made the principles much clearer than reading documentation alone.\nVietnamese Enterprise Context: Learning about common security pitfalls in Vietnamese enterprises was particularly insightful. Understanding local context helps identify relevant security concerns and solutions.\nActionable Playbooks: The incident response playbooks provided templates that can be immediately applied. Having structured procedures for common scenarios is invaluable for effective incident response.\nPillar Interconnections: Understanding how all five pillars work together helped me see security as a comprehensive system rather than isolated controls. This holistic view is essential for designing effective security architectures.\nKey Realizations Security is Continuous: The workshop reinforced that security is a continuous journey, not a destination. Regular reviews, updates, and improvements are essential.\nAutomation is Essential: Automation emerged as a critical theme across all pillars. Manual security processes don\u0026rsquo;t scale and are prone to errors.\nFramework Value: The Well-Architected Security Pillar provides a comprehensive framework that covers all aspects of cloud security systematically.\nNative AWS Tools: AWS provides powerful native tools for each security domain, making it possible to build comprehensive security without third-party solutions.\nImpact on My Career This workshop has significantly expanded my understanding of cloud security and how to implement it effectively on AWS. The practical, hands-on approach provided immediately applicable knowledge. The workshop has inspired me to:\nDeepen my security expertise Pursue security certifications Contribute to building secure cloud architectures Stay current with evolving security threats and solutions Takeaways Strategic Principles Security is a Journey: Security is a continuous process, not a one-time implementation. Regular reviews and improvements are essential.\nAutomation is Key: Automation is essential for maintaining security at scale. Manual processes are error-prone and don\u0026rsquo;t scale effectively.\nComprehensive Framework: The Well-Architected Security Pillar provides a complete framework for cloud security covering all necessary domains.\nRegular Reviews: Periodic security reviews and improvements are essential for maintaining effective security posture.\nNative Tools: AWS provides powerful native tools for each security domain, enabling comprehensive security without third-party dependencies.\nEvent Photos Add your event photos here\nThe AWS Well-Architected Security Pillar Workshop was a comprehensive and practical introduction to cloud security on AWS. The systematic coverage of all five security pillars, combined with practical demonstrations and real-world examples, provided a solid foundation for implementing security best practices. The workshop reinforced that security requires a holistic approach, with all pillars working together to create effective defense-in-depth. The insights gained from this workshop have equipped me with the knowledge and skills needed to design and implement secure cloud architectures, and have inspired me to continue developing my security expertise throughout my career.\n"
},
{
	"uri": "http://localhost:1313/aws_ojt_report-TranThiThuHa/",
	"title": "Internship Report",
	"tags": [],
	"description": "",
	"content": "Internship Report ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nStudent Information: Full Name: Nguyen Van A\nPhone Number: 0989888999\nEmail: Anguyenvan@gmail.com\nUniversity: Ho Chi Minh City University of Technology and Education\nMajor: Information Technology\nClass: AWS082025\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 12/08/2025 to 12/11/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "
},
{
	"uri": "http://localhost:1313/aws_ojt_report-TranThiThuHa/5-workshop/5.1-workshop-overview/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "VPC endpoints VPC endpoints are virtual devices. They are horizontally scaled, redundant, and highly available VPC components. They allow communication between your compute resources and AWS services without imposing availability risks. Compute resources running in VPC can access Amazon S3 using a Gateway endpoint. PrivateLink interface endpoints can be used by compute resources running in VPC or on-premises. Workshop overview In this workshop, you will use two VPCs.\n\u0026ldquo;VPC Cloud\u0026rdquo; is for cloud resources such as a Gateway endpoint and an EC2 instance to test with. \u0026ldquo;VPC On-Prem\u0026rdquo; simulates an on-premises environment such as a factory or corporate datacenter. An EC2 instance running strongSwan VPN software has been deployed in \u0026ldquo;VPC On-prem\u0026rdquo; and automatically configured to establish a Site-to-Site VPN tunnel with AWS Transit Gateway. This VPN simulates connectivity from an on-premises location to the AWS cloud. To minimize costs, only one VPN instance is provisioned to support this workshop. When planning VPN connectivity for your production workloads, AWS recommends using multiple VPN devices for high availability. "
},
{
	"uri": "http://localhost:1313/aws_ojt_report-TranThiThuHa/5-workshop/5.4-s3-onprem/5.4.1-prepare/",
	"title": "Prepare the environment",
	"tags": [],
	"description": "",
	"content": "To prepare for this part of the workshop you will need to:\nDeploying a CloudFormation stack Modifying a VPC route table. These components work together to simulate on-premises DNS forwarding and name resolution.\nDeploy the CloudFormation stack The CloudFormation template will create additional services to support an on-premises simulation:\nOne Route 53 Private Hosted Zone that hosts Alias records for the PrivateLink S3 endpoint One Route 53 Inbound Resolver endpoint that enables \u0026ldquo;VPC Cloud\u0026rdquo; to resolve inbound DNS resolution requests to the Private Hosted Zone One Route 53 Outbound Resolver endpoint that enables \u0026ldquo;VPC On-prem\u0026rdquo; to forward DNS requests for S3 to \u0026ldquo;VPC Cloud\u0026rdquo; Click the following link to open the AWS CloudFormation console. The required template will be pre-loaded into the menu. Accept all default and click Create stack. It may take a few minutes for stack deployment to complete. You can continue with the next step without waiting for the deployemnt to finish.\nUpdate on-premise private route table This workshop uses a strongSwan VPN running on an EC2 instance to simulate connectivty between an on-premises datacenter and the AWS cloud. Most of the required components are provisioned before your start. To finalize the VPN configuration, you will modify the \u0026ldquo;VPC On-prem\u0026rdquo; routing table to direct traffic destined for the cloud to the strongSwan VPN instance.\nOpen the Amazon EC2 console\nSelect the instance named infra-vpngw-test. From the Details tab, copy the Instance ID and paste this into your text editor\nNavigate to the VPC menu by using the Search box at the top of the browser window.\nClick on Route Tables, select the RT Private On-prem route table, select the Routes tab, and click Edit Routes.\nClick Add route. Destination: your Cloud VPC cidr range Target: ID of your infra-vpngw-test instance (you saved in your editor at step 1) Click Save changes "
},
{
	"uri": "http://localhost:1313/aws_ojt_report-TranThiThuHa/1-worklog/1.1-week1/",
	"title": "Week 1 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 1 Objectives: Get acquainted with the internship environment and members of First Cloud Journey. Understand AWS overview and basic services. Create an AWS account and set up cost management. Master AWS IAM for access management and security. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material Mon - Get acquainted with FCJ members - Read and take note of internship rules and regulations - Introduction to Cloud Engineer learning path 08/09/2025 08/09/2025 https://cloudjourney.awsstudygroup.com/ Tue - Learn AWS overview + What is AWS? + Main service categories + Shared Responsibility Model - Create AWS Free Tier account 09/09/2025 09/09/2025 https://000001.awsstudygroup.com/ Wed - Learn AWS Budgets and cost management - Practice: + Set up AWS Budgets + Configure cost alerts + Use Cost Explorer 10/09/2025 10/09/2025 https://000007.awsstudygroup.com/ Thu - Learn AWS IAM: + Users, Groups, Roles + Policies and Permissions + Security best practices - Practice: Create IAM User, Group and assign Policies 11/09/2025 11/09/2025 https://000002.awsstudygroup.com/ Fri - Learn AWS Console \u0026amp; AWS CLI - Practice: + Install and configure AWS CLI + Use CLI with IAM credentials + Basic CLI operations 12/09/2025 12/09/2025 https://000011.awsstudygroup.com/ Week 1 Achievements: Completed orientation with the internship environment and understood the workflow at FCJ.\nUnderstood AWS overview:\nAWS is the leading cloud platform with over 200 services Main service categories: Compute, Storage, Database, Networking, Security,\u0026hellip; Shared Responsibility Model AWS Regions and Availability Zones Successfully created and activated AWS Free Tier account.\nSet up cost management with AWS Budgets:\nCreated budget with alert thresholds Configured email notifications when costs exceed thresholds Used Cost Explorer to track spending Mastered AWS IAM and practiced:\nCreated IAM User with programmatic access Created IAM Group and assigned Users to Group Set up IAM Policies to manage access permissions Enabled MFA for root account and IAM User Successfully installed and configured AWS CLI:\nInstalled AWS CLI v2 on computer Configured credentials with Access Key and Secret Key Set default region Executed basic commands: aws sts get-caller-identity, aws iam list-users,\u0026hellip; Learned to use AWS Console (web interface) and AWS CLI in parallel to manage AWS resources.\n"
},
{
	"uri": "http://localhost:1313/aws_ojt_report-TranThiThuHa/1-worklog/",
	"title": "Worklog",
	"tags": [],
	"description": "",
	"content": "AWS Cloud Engineer Internship Worklog - 12 weeks following the learning path at cloudjourney.awsstudygroup.com\nExplore AWS Services (Week 1-5) Week 1: AWS Account, Budgets, Support \u0026amp; IAM Basics\nWeek 2: Amazon VPC, EC2 \u0026amp; IAM Roles for EC2\nWeek 3: AWS Cloud9, Amazon S3 \u0026amp; Amazon RDS\nWeek 4: Amazon Lightsail, EC2 Auto Scaling, CloudWatch \u0026amp; Route 53\nWeek 5: DynamoDB, ElastiCache, CloudFront \u0026amp; Lambda@Edge\nOptimizing - Operations (Week 6-8) Week 6: AWS Lambda, CloudWatch + Grafana, Tags \u0026amp; Systems Manager\nWeek 7: AWS CloudFormation, CDK \u0026amp; EC2 Right-Sizing\nWeek 8: Session Manager, VPC Flow Logs, Service Quotas \u0026amp; Cost Management\nOptimizing - Security (Week 9-11) Week 9: AWS SSO, IAM Permission Boundaries, Security Hub \u0026amp; VPC Endpoints\nWeek 10: AWS WAF, KMS, Macie, Secrets Manager \u0026amp; Firewall Manager\nWeek 11: GuardDuty, EC2 Image Builder, Cognito, S3 Security \u0026amp; AWS Backup\nFinal Project \u0026amp; Summary (Week 12) Week 12: Final Project - Building Complete AWS Architecture\n"
},
{
	"uri": "http://localhost:1313/aws_ojt_report-TranThiThuHa/5-workshop/5.4-s3-onprem/5.4.2-create-interface-enpoint/",
	"title": "Create an S3 Interface endpoint",
	"tags": [],
	"description": "",
	"content": "In this section you will create and test an S3 interface endpoint using the simulated on-premises environment deployed as part of this workshop.\nReturn to the Amazon VPC menu. In the navigation pane, choose Endpoints, then click Create Endpoint.\nIn Create endpoint console:\nName the interface endpoint In Service category, choose aws services In the Search box, type S3 and press Enter. Select the endpoint named com.amazonaws.us-east-1.s3. Ensure that the Type column indicates Interface. For VPC, select VPC Cloud from the drop-down. Make sure to choose \u0026ldquo;VPC Cloud\u0026rdquo; and not \u0026ldquo;VPC On-prem\u0026rdquo;\nExpand Additional settings and ensure that Enable DNS name is not selected (we will use this in the next part of the workshop) Select 2 subnets in the following AZs: us-east-1a and us-east-1b For Security group, choose SGforS3Endpoint: Keep the default policy - full access and click Create endpoint Congratulation on successfully creating S3 interface endpoint. In the next step, we will test the interface endpoint.\n"
},
{
	"uri": "http://localhost:1313/aws_ojt_report-TranThiThuHa/5-workshop/5.2-prerequiste/",
	"title": "Prerequiste",
	"tags": [],
	"description": "",
	"content": "IAM permissions Add the following IAM permission policy to your user account to deploy and cleanup this workshop.\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;VisualEditor0\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;cloudformation:*\u0026#34;, \u0026#34;cloudwatch:*\u0026#34;, \u0026#34;ec2:AcceptTransitGatewayPeeringAttachment\u0026#34;, \u0026#34;ec2:AcceptTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:AllocateAddress\u0026#34;, \u0026#34;ec2:AssociateAddress\u0026#34;, \u0026#34;ec2:AssociateIamInstanceProfile\u0026#34;, \u0026#34;ec2:AssociateRouteTable\u0026#34;, \u0026#34;ec2:AssociateSubnetCidrBlock\u0026#34;, \u0026#34;ec2:AssociateTransitGatewayRouteTable\u0026#34;, \u0026#34;ec2:AssociateVpcCidrBlock\u0026#34;, \u0026#34;ec2:AttachInternetGateway\u0026#34;, \u0026#34;ec2:AttachNetworkInterface\u0026#34;, \u0026#34;ec2:AttachVolume\u0026#34;, \u0026#34;ec2:AttachVpnGateway\u0026#34;, \u0026#34;ec2:AuthorizeSecurityGroupEgress\u0026#34;, \u0026#34;ec2:AuthorizeSecurityGroupIngress\u0026#34;, \u0026#34;ec2:CreateClientVpnEndpoint\u0026#34;, \u0026#34;ec2:CreateClientVpnRoute\u0026#34;, \u0026#34;ec2:CreateCustomerGateway\u0026#34;, \u0026#34;ec2:CreateDhcpOptions\u0026#34;, \u0026#34;ec2:CreateFlowLogs\u0026#34;, \u0026#34;ec2:CreateInternetGateway\u0026#34;, \u0026#34;ec2:CreateLaunchTemplate\u0026#34;, \u0026#34;ec2:CreateNetworkAcl\u0026#34;, \u0026#34;ec2:CreateNetworkInterface\u0026#34;, \u0026#34;ec2:CreateNetworkInterfacePermission\u0026#34;, \u0026#34;ec2:CreateRoute\u0026#34;, \u0026#34;ec2:CreateRouteTable\u0026#34;, \u0026#34;ec2:CreateSecurityGroup\u0026#34;, \u0026#34;ec2:CreateSubnet\u0026#34;, \u0026#34;ec2:CreateSubnetCidrReservation\u0026#34;, \u0026#34;ec2:CreateTags\u0026#34;, \u0026#34;ec2:CreateTransitGateway\u0026#34;, \u0026#34;ec2:CreateTransitGatewayPeeringAttachment\u0026#34;, \u0026#34;ec2:CreateTransitGatewayPrefixListReference\u0026#34;, \u0026#34;ec2:CreateTransitGatewayRoute\u0026#34;, \u0026#34;ec2:CreateTransitGatewayRouteTable\u0026#34;, \u0026#34;ec2:CreateTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:CreateVpc\u0026#34;, \u0026#34;ec2:CreateVpcEndpoint\u0026#34;, \u0026#34;ec2:CreateVpcEndpointConnectionNotification\u0026#34;, \u0026#34;ec2:CreateVpcEndpointServiceConfiguration\u0026#34;, \u0026#34;ec2:CreateVpnConnection\u0026#34;, \u0026#34;ec2:CreateVpnConnectionRoute\u0026#34;, \u0026#34;ec2:CreateVpnGateway\u0026#34;, \u0026#34;ec2:DeleteCustomerGateway\u0026#34;, \u0026#34;ec2:DeleteFlowLogs\u0026#34;, \u0026#34;ec2:DeleteInternetGateway\u0026#34;, \u0026#34;ec2:DeleteNetworkInterface\u0026#34;, \u0026#34;ec2:DeleteNetworkInterfacePermission\u0026#34;, \u0026#34;ec2:DeleteRoute\u0026#34;, \u0026#34;ec2:DeleteRouteTable\u0026#34;, \u0026#34;ec2:DeleteSecurityGroup\u0026#34;, \u0026#34;ec2:DeleteSubnet\u0026#34;, \u0026#34;ec2:DeleteSubnetCidrReservation\u0026#34;, \u0026#34;ec2:DeleteTags\u0026#34;, \u0026#34;ec2:DeleteTransitGateway\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayPeeringAttachment\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayPrefixListReference\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayRoute\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayRouteTable\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:DeleteVpc\u0026#34;, \u0026#34;ec2:DeleteVpcEndpoints\u0026#34;, \u0026#34;ec2:DeleteVpcEndpointServiceConfigurations\u0026#34;, \u0026#34;ec2:DeleteVpnConnection\u0026#34;, \u0026#34;ec2:DeleteVpnConnectionRoute\u0026#34;, \u0026#34;ec2:Describe*\u0026#34;, \u0026#34;ec2:DetachInternetGateway\u0026#34;, \u0026#34;ec2:DisassociateAddress\u0026#34;, \u0026#34;ec2:DisassociateRouteTable\u0026#34;, \u0026#34;ec2:GetLaunchTemplateData\u0026#34;, \u0026#34;ec2:GetTransitGatewayAttachmentPropagations\u0026#34;, \u0026#34;ec2:ModifyInstanceAttribute\u0026#34;, \u0026#34;ec2:ModifySecurityGroupRules\u0026#34;, \u0026#34;ec2:ModifyTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:ModifyVpcAttribute\u0026#34;, \u0026#34;ec2:ModifyVpcEndpoint\u0026#34;, \u0026#34;ec2:ReleaseAddress\u0026#34;, \u0026#34;ec2:ReplaceRoute\u0026#34;, \u0026#34;ec2:RevokeSecurityGroupEgress\u0026#34;, \u0026#34;ec2:RevokeSecurityGroupIngress\u0026#34;, \u0026#34;ec2:RunInstances\u0026#34;, \u0026#34;ec2:StartInstances\u0026#34;, \u0026#34;ec2:StopInstances\u0026#34;, \u0026#34;ec2:UpdateSecurityGroupRuleDescriptionsEgress\u0026#34;, \u0026#34;ec2:UpdateSecurityGroupRuleDescriptionsIngress\u0026#34;, \u0026#34;iam:AddRoleToInstanceProfile\u0026#34;, \u0026#34;iam:AttachRolePolicy\u0026#34;, \u0026#34;iam:CreateInstanceProfile\u0026#34;, \u0026#34;iam:CreatePolicy\u0026#34;, \u0026#34;iam:CreateRole\u0026#34;, \u0026#34;iam:DeleteInstanceProfile\u0026#34;, \u0026#34;iam:DeletePolicy\u0026#34;, \u0026#34;iam:DeleteRole\u0026#34;, \u0026#34;iam:DeleteRolePolicy\u0026#34;, \u0026#34;iam:DetachRolePolicy\u0026#34;, \u0026#34;iam:GetInstanceProfile\u0026#34;, \u0026#34;iam:GetPolicy\u0026#34;, \u0026#34;iam:GetRole\u0026#34;, \u0026#34;iam:GetRolePolicy\u0026#34;, \u0026#34;iam:ListPolicyVersions\u0026#34;, \u0026#34;iam:ListRoles\u0026#34;, \u0026#34;iam:PassRole\u0026#34;, \u0026#34;iam:PutRolePolicy\u0026#34;, \u0026#34;iam:RemoveRoleFromInstanceProfile\u0026#34;, \u0026#34;lambda:CreateFunction\u0026#34;, \u0026#34;lambda:DeleteFunction\u0026#34;, \u0026#34;lambda:DeleteLayerVersion\u0026#34;, \u0026#34;lambda:GetFunction\u0026#34;, \u0026#34;lambda:GetLayerVersion\u0026#34;, \u0026#34;lambda:InvokeFunction\u0026#34;, \u0026#34;lambda:PublishLayerVersion\u0026#34;, \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;logs:DeleteLogGroup\u0026#34;, \u0026#34;logs:DescribeLogGroups\u0026#34;, \u0026#34;logs:PutRetentionPolicy\u0026#34;, \u0026#34;route53:ChangeTagsForResource\u0026#34;, \u0026#34;route53:CreateHealthCheck\u0026#34;, \u0026#34;route53:CreateHostedZone\u0026#34;, \u0026#34;route53:CreateTrafficPolicy\u0026#34;, \u0026#34;route53:DeleteHostedZone\u0026#34;, \u0026#34;route53:DisassociateVPCFromHostedZone\u0026#34;, \u0026#34;route53:GetHostedZone\u0026#34;, \u0026#34;route53:ListHostedZones\u0026#34;, \u0026#34;route53domains:ListDomains\u0026#34;, \u0026#34;route53domains:ListOperations\u0026#34;, \u0026#34;route53domains:ListTagsForDomain\u0026#34;, \u0026#34;route53resolver:AssociateResolverEndpointIpAddress\u0026#34;, \u0026#34;route53resolver:AssociateResolverRule\u0026#34;, \u0026#34;route53resolver:CreateResolverEndpoint\u0026#34;, \u0026#34;route53resolver:CreateResolverRule\u0026#34;, \u0026#34;route53resolver:DeleteResolverEndpoint\u0026#34;, \u0026#34;route53resolver:DeleteResolverRule\u0026#34;, \u0026#34;route53resolver:DisassociateResolverEndpointIpAddress\u0026#34;, \u0026#34;route53resolver:DisassociateResolverRule\u0026#34;, \u0026#34;route53resolver:GetResolverEndpoint\u0026#34;, \u0026#34;route53resolver:GetResolverRule\u0026#34;, \u0026#34;route53resolver:ListResolverEndpointIpAddresses\u0026#34;, \u0026#34;route53resolver:ListResolverEndpoints\u0026#34;, \u0026#34;route53resolver:ListResolverRuleAssociations\u0026#34;, \u0026#34;route53resolver:ListResolverRules\u0026#34;, \u0026#34;route53resolver:ListTagsForResource\u0026#34;, \u0026#34;route53resolver:UpdateResolverEndpoint\u0026#34;, \u0026#34;route53resolver:UpdateResolverRule\u0026#34;, \u0026#34;s3:AbortMultipartUpload\u0026#34;, \u0026#34;s3:CreateBucket\u0026#34;, \u0026#34;s3:DeleteBucket\u0026#34;, \u0026#34;s3:DeleteObject\u0026#34;, \u0026#34;s3:GetAccountPublicAccessBlock\u0026#34;, \u0026#34;s3:GetBucketAcl\u0026#34;, \u0026#34;s3:GetBucketOwnershipControls\u0026#34;, \u0026#34;s3:GetBucketPolicy\u0026#34;, \u0026#34;s3:GetBucketPolicyStatus\u0026#34;, \u0026#34;s3:GetBucketPublicAccessBlock\u0026#34;, \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:GetObjectVersion\u0026#34;, \u0026#34;s3:GetBucketVersioning\u0026#34;, \u0026#34;s3:ListAccessPoints\u0026#34;, \u0026#34;s3:ListAccessPointsForObjectLambda\u0026#34;, \u0026#34;s3:ListAllMyBuckets\u0026#34;, \u0026#34;s3:ListBucket\u0026#34;, \u0026#34;s3:ListBucketMultipartUploads\u0026#34;, \u0026#34;s3:ListBucketVersions\u0026#34;, \u0026#34;s3:ListJobs\u0026#34;, \u0026#34;s3:ListMultipartUploadParts\u0026#34;, \u0026#34;s3:ListMultiRegionAccessPoints\u0026#34;, \u0026#34;s3:ListStorageLensConfigurations\u0026#34;, \u0026#34;s3:PutAccountPublicAccessBlock\u0026#34;, \u0026#34;s3:PutBucketAcl\u0026#34;, \u0026#34;s3:PutBucketPolicy\u0026#34;, \u0026#34;s3:PutBucketPublicAccessBlock\u0026#34;, \u0026#34;s3:PutObject\u0026#34;, \u0026#34;secretsmanager:CreateSecret\u0026#34;, \u0026#34;secretsmanager:DeleteSecret\u0026#34;, \u0026#34;secretsmanager:DescribeSecret\u0026#34;, \u0026#34;secretsmanager:GetSecretValue\u0026#34;, \u0026#34;secretsmanager:ListSecrets\u0026#34;, \u0026#34;secretsmanager:ListSecretVersionIds\u0026#34;, \u0026#34;secretsmanager:PutResourcePolicy\u0026#34;, \u0026#34;secretsmanager:TagResource\u0026#34;, \u0026#34;secretsmanager:UpdateSecret\u0026#34;, \u0026#34;sns:ListTopics\u0026#34;, \u0026#34;ssm:DescribeInstanceProperties\u0026#34;, \u0026#34;ssm:DescribeSessions\u0026#34;, \u0026#34;ssm:GetConnectionStatus\u0026#34;, \u0026#34;ssm:GetParameters\u0026#34;, \u0026#34;ssm:ListAssociations\u0026#34;, \u0026#34;ssm:ResumeSession\u0026#34;, \u0026#34;ssm:StartSession\u0026#34;, \u0026#34;ssm:TerminateSession\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } Provision resources using CloudFormation In this lab, we will use N.Virginia region (us-east-1).\nTo prepare the workshop environment, deploy this CloudFormation Template (click link): PrivateLinkWorkshop . Accept all of the defaults when deploying the template.\nTick 2 acknowledgement boxes Choose Create stack The ClouddFormation deployment requires about 15 minutes to complete.\n2 VPCs have been created 3 EC2s have been created "
},
{
	"uri": "http://localhost:1313/aws_ojt_report-TranThiThuHa/2-proposal/",
	"title": "Proposal",
	"tags": [],
	"description": "",
	"content": " ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nIn this section, you need to summarize the contents of the workshop that you plan to conduct.\nIoT Weather Platform for Lab Research A Unified AWS Serverless Solution for Real-Time Weather Monitoring 1. Executive Summary The IoT Weather Platform is designed for the ITea Lab team in Ho Chi Minh City to enhance weather data collection and analysis. It supports up to 5 weather stations, with potential scalability to 10-15, utilizing Raspberry Pi edge devices with ESP32 sensors to transmit data via MQTT. The platform leverages AWS Serverless services to deliver real-time monitoring, predictive analytics, and cost efficiency, with access restricted to 5 lab members via Amazon Cognito.\n2. Problem Statement What’s the Problem? Current weather stations require manual data collection, becoming unmanageable with multiple units. There is no centralized system for real-time data or analytics, and third-party platforms are costly and overly complex.\nThe Solution The platform uses AWS IoT Core to ingest MQTT data, AWS Lambda and API Gateway for processing, Amazon S3 for storage (including a data lake), and AWS Glue Crawlers and ETL jobs to extract, transform, and load data from the S3 data lake to another S3 bucket for analysis. AWS Amplify with Next.js provides the web interface, and Amazon Cognito ensures secure access. Similar to Thingsboard and CoreIoT, users can register new devices and manage connections, though this platform operates on a smaller scale and is designed for private use. Key features include real-time dashboards, trend analysis, and low operational costs.\nBenefits and Return on Investment The solution establishes a foundational resource for lab members to develop a larger IoT platform, serving as a study resource, and provides a data foundation for AI enthusiasts for model training or analysis. It reduces manual reporting for each station via a centralized platform, simplifying management and maintenance, and improves data reliability. Monthly costs are $0.66 USD per the AWS Pricing Calculator, with a 12-month total of $7.92 USD. All IoT equipment costs are covered by the existing weather station setup, eliminating additional development expenses. The break-even period of 6-12 months is achieved through significant time savings from reduced manual work.\n3. Solution Architecture The platform employs a serverless AWS architecture to manage data from 5 Raspberry Pi-based stations, scalable to 15. Data is ingested via AWS IoT Core, stored in an S3 data lake, and processed by AWS Glue Crawlers and ETL jobs to transform and load it into another S3 bucket for analysis. Lambda and API Gateway handle additional processing, while Amplify with Next.js hosts the dashboard, secured by Cognito. The architecture is detailed below:\nAWS Services Used AWS IoT Core: Ingests MQTT data from 5 stations, scalable to 15. AWS Lambda: Processes data and triggers Glue jobs (two functions). Amazon API Gateway: Facilitates web app communication. Amazon S3: Stores raw data in a data lake and processed outputs (two buckets). AWS Glue: Crawlers catalog data, and ETL jobs transform and load it. AWS Amplify: Hosts the Next.js web interface. Amazon Cognito: Secures access for lab users. Component Design Edge Devices: Raspberry Pi collects and filters sensor data, sending it to IoT Core. Data Ingestion: AWS IoT Core receives MQTT messages from the edge devices. Data Storage: Raw data is stored in an S3 data lake; processed data is stored in another S3 bucket. Data Processing: AWS Glue Crawlers catalog the data, and ETL jobs transform it for analysis. Web Interface: AWS Amplify hosts a Next.js app for real-time dashboards and analytics. User Management: Amazon Cognito manages user access, allowing up to 5 active accounts. 4. Technical Implementation Implementation Phases This project has two parts—setting up weather edge stations and building the weather platform—each following 4 phases:\nBuild Theory and Draw Architecture: Research Raspberry Pi setup with ESP32 sensors and design the AWS serverless architecture (1 month pre-internship) Calculate Price and Check Practicality: Use AWS Pricing Calculator to estimate costs and adjust if needed (Month 1). Fix Architecture for Cost or Solution Fit: Tweak the design (e.g., optimize Lambda with Next.js) to stay cost-effective and usable (Month 2). Develop, Test, and Deploy: Code the Raspberry Pi setup, AWS services with CDK/SDK, and Next.js app, then test and release to production (Months 2-3). Technical Requirements\nWeather Edge Station: Sensors (temperature, humidity, rainfall, wind speed), a microcontroller (ESP32), and a Raspberry Pi as the edge device. Raspberry Pi runs Raspbian, handles Docker for filtering, and sends 1 MB/day per station via MQTT over Wi-Fi. Weather Platform: Practical knowledge of AWS Amplify (hosting Next.js), Lambda (minimal use due to Next.js), AWS Glue (ETL), S3 (two buckets), IoT Core (gateway and rules), and Cognito (5 users). Use AWS CDK/SDK to code interactions (e.g., IoT Core rules to S3). Next.js reduces Lambda workload for the fullstack web app. 5. Timeline \u0026amp; Milestones Project Timeline\nPre-Internship (Month 0): 1 month for planning and old station review. Internship (Months 1-3): 3 months. Month 1: Study AWS and upgrade hardware. Month 2: Design and adjust architecture. Month 3: Implement, test, and launch. Post-Launch: Up to 1 year for research. 6. Budget Estimation You can find the budget estimation on the AWS Pricing Calculator.\nOr you can download the Budget Estimation File.\nInfrastructure Costs AWS Services: AWS Lambda: $0.00/month (1,000 requests, 512 MB storage). S3 Standard: $0.15/month (6 GB, 2,100 requests, 1 GB scanned). Data Transfer: $0.02/month (1 GB inbound, 1 GB outbound). AWS Amplify: $0.35/month (256 MB, 500 ms requests). Amazon API Gateway: $0.01/month (2,000 requests). AWS Glue ETL Jobs: $0.02/month (2 DPUs). AWS Glue Crawlers: $0.07/month (1 crawler). MQTT (IoT Core): $0.08/month (5 devices, 45,000 messages). Total: $0.7/month, $8.40/12 months\nHardware: $265 one-time (Raspberry Pi 5 and sensors). 7. Risk Assessment Risk Matrix Network Outages: Medium impact, medium probability. Sensor Failures: High impact, low probability. Cost Overruns: Medium impact, low probability. Mitigation Strategies Network: Local storage on Raspberry Pi with Docker. Sensors: Regular checks and spares. Cost: AWS budget alerts and optimization. Contingency Plans Revert to manual methods if AWS fails. Use CloudFormation for cost-related rollbacks. 8. Expected Outcomes Technical Improvements: Real-time data and analytics replace manual processes.\nScalable to 10-15 stations.\nLong-term Value 1-year data foundation for AI research.\nReusable for future projects.\n"
},
{
	"uri": "http://localhost:1313/aws_ojt_report-TranThiThuHa/5-workshop/5.3-s3-vpc/5.3.2-test-gwe/",
	"title": "Test the Gateway Endpoint",
	"tags": [],
	"description": "",
	"content": "Create S3 bucket Navigate to S3 management console In the Bucket console, choose Create bucket In the Create bucket console Name the bucket: choose a name that hasn\u0026rsquo;t been given to any bucket globally (hint: lab number and your name) Leave other fields as they are (default) Scroll down and choose Create bucket Successfully create S3 bucket. Connect to EC2 with session manager For this workshop, you will use AWS Session Manager to access several EC2 instances. Session Manager is a fully managed AWS Systems Manager capability that allows you to manage your Amazon EC2 instances and on-premises virtual machines (VMs) through an interactive one-click browser-based shell. Session Manager provides secure and auditable instance management without the need to open inbound ports, maintain bastion hosts, or manage SSH keys.\nFirst cloud journey Lab for indepth understanding of Session manager.\nIn the AWS Management Console, start typing Systems Manager in the quick search box and press Enter: From the Systems Manager menu, find Node Management in the left menu and click Session Manager: Click Start Session, and select the EC2 instance named Test-Gateway-Endpoint. This EC2 instance is already running in \u0026ldquo;VPC Cloud\u0026rdquo; and will be used to test connectivity to Amazon S3 through the Gateway endpoint you just created (s3-gwe).\nSession Manager will open a new browser tab with a shell prompt: sh-4.2 $\nYou have successfully start a session - connect to the EC2 instance in VPC cloud. In the next step, we will create a S3 bucket and a file in it.\nCreate a file and upload to s3 bucket Change to the ssm-user\u0026rsquo;s home directory by typing cd ~ in the CLI Create a new file to use for testing with the command fallocate -l 1G testfile.xyz, which will create a file of 1GB size named \u0026ldquo;testfile.xyz\u0026rdquo;. Upload file to S3 bucket with command aws s3 cp testfile.xyz s3://your-bucket-name. Replace your-bucket-name with the name of S3 bucket that you created earlier. You have successfully uploaded the file to your S3 bucket. You can now terminate the session.\nCheck object in S3 bucket Navigate to S3 console. Click the name of your s3 bucket In the Bucket console, you will see the file you have uploaded to your S3 bucket Section summary Congratulation on completing access to S3 from VPC. In this section, you created a Gateway endpoint for Amazon S3, and used the AWS CLI to upload an object. The upload worked because the Gateway endpoint allowed communication to S3, without needing an Internet Gateway attached to \u0026ldquo;VPC Cloud\u0026rdquo;. This demonstrates the functionality of the Gateway endpoint as a secure path to S3 without traversing the Public Internet.\n"
},
{
	"uri": "http://localhost:1313/aws_ojt_report-TranThiThuHa/1-worklog/1.2-week2/",
	"title": "Week 2 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 2 Objectives: Understand networking fundamentals with Amazon VPC. Learn compute essentials with Amazon EC2. Practice launching and managing EC2 instances. Understand IAM Roles for EC2 instances. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material Mon - Learn Amazon VPC fundamentals: + VPC, Subnets (Public/Private) + Internet Gateway, NAT Gateway + Route Tables - Practice: Create a custom VPC 15/09/2025 15/09/2025 https://000003.awsstudygroup.com/ Tue - Learn VPC Security: + Security Groups + Network ACLs + VPC Flow Logs - Practice: Configure Security Groups and NACLs 16/09/2025 16/09/2025 https://000003.awsstudygroup.com/ Wed - Learn Amazon EC2 basics: + Instance types \u0026amp; families + Amazon Machine Images (AMI) + Instance lifecycle + Pricing options 17/09/2025 17/09/2025 https://000004.awsstudygroup.com/ Thu - Learn EC2 Storage \u0026amp; Networking: + EBS volumes \u0026amp; snapshots + Elastic IP + Key Pairs - Practice: Launch EC2 instance \u0026amp; connect via SSH 18/09/2025 18/09/2025 https://000004.awsstudygroup.com/ Fri - Learn IAM Roles for EC2: + Instance profiles + Assigning roles to EC2 - Practice: + Create IAM Role for EC2 + Access S3 from EC2 using Role 19/09/2025 19/09/2025 https://000048.awsstudygroup.com/ Week 2 Achievements: Mastered Amazon VPC networking concepts:\nCreated custom VPC with CIDR block Configured Public and Private Subnets Set up Internet Gateway for public internet access Configured Route Tables for traffic routing Understood NAT Gateway for private subnet internet access Implemented VPC security:\nConfigured Security Groups (stateful firewall) Set up Network ACLs (stateless firewall) Understood inbound/outbound rules Understood Amazon EC2 fundamentals:\nDifferent instance types and use cases (t2, t3, m5, c5,\u0026hellip;) AMI selection and creation Instance lifecycle: pending, running, stopping, terminated Pricing: On-Demand, Reserved, Spot instances Successfully launched and managed EC2 instances:\nLaunched EC2 instance in custom VPC Connected to instance via SSH using Key Pair Attached and managed EBS volumes Assigned Elastic IP to instance Configured IAM Roles for EC2:\nCreated IAM Role with appropriate policies Attached Instance Profile to EC2 Accessed AWS services (S3) from EC2 without hardcoded credentials Understood security benefits of using IAM Roles vs Access Keys "
},
{
	"uri": "http://localhost:1313/aws_ojt_report-TranThiThuHa/5-workshop/5.3-s3-vpc/",
	"title": "Access S3 from VPC",
	"tags": [],
	"description": "",
	"content": "Using Gateway endpoint In this section, you will create a Gateway eendpoint to access Amazon S3 from an EC2 instance. The Gateway endpoint will allow upload an object to S3 buckets without using the Public Internet. To create an endpoint, you must specify the VPC in which you want to create the endpoint, and the service (in this case, S3) to which you want to establish the connection.\nContent Create gateway endpoint Test gateway endpoint "
},
{
	"uri": "http://localhost:1313/aws_ojt_report-TranThiThuHa/5-workshop/5.4-s3-onprem/5.4.3-test-endpoint/",
	"title": "Test the Interface Endpoint",
	"tags": [],
	"description": "",
	"content": "Get the regional DNS name of S3 interface endpoint From the Amazon VPC menu, choose Endpoints.\nClick the name of newly created endpoint: s3-interface-endpoint. Click details and save the regional DNS name of the endpoint (the first one) to your text-editor for later use.\nConnect to EC2 instance in \u0026ldquo;VPC On-prem\u0026rdquo; Navigate to Session manager by typing \u0026ldquo;session manager\u0026rdquo; in the search box\nClick Start Session, and select the EC2 instance named Test-Interface-Endpoint. This EC2 instance is running in \u0026ldquo;VPC On-prem\u0026rdquo; and will be used to test connectivty to Amazon S3 through the Interface endpoint we just created. Session Manager will open a new browser tab with a shell prompt: sh-4.2 $\nChange to the ssm-user\u0026rsquo;s home directory with command \u0026ldquo;cd ~\u0026rdquo;\nCreate a file named testfile2.xyz\nfallocate -l 1G testfile2.xyz Copy file to the same S3 bucket we created in section 3.2 aws s3 cp --endpoint-url https://bucket.\u0026lt;Regional-DNS-Name\u0026gt; testfile2.xyz s3://\u0026lt;your-bucket-name\u0026gt; This command requires the \u0026ndash;endpoint-url parameter, because you need to use the endpoint-specific DNS name to access S3 using an Interface endpoint. Do not include the leading \u0026rsquo; * \u0026rsquo; when copying/pasting the regional DNS name. Provide your S3 bucket name created earlier Now the file has been added to your S3 bucket. Let check your S3 bucket in the next step.\nCheck Object in S3 bucket Navigate to S3 console Click Buckets Click the name of your bucket and you will see testfile2.xyz has been added to your bucket "
},
{
	"uri": "http://localhost:1313/aws_ojt_report-TranThiThuHa/1-worklog/1.3-week3/",
	"title": "Week 3 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 3 Objectives: Learn cloud development environment with AWS Cloud9. Master object storage with Amazon S3. Host a static website on Amazon S3. Understand database fundamentals with Amazon RDS. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material Mon - Learn AWS Cloud9: + Cloud-based IDE features + Environment setup + Integrated terminal \u0026amp; debugging - Practice: Create Cloud9 environment 22/09/2025 22/09/2025 https://000049.awsstudygroup.com/ Tue - Learn Amazon S3 basics: + Buckets and Objects + Storage classes + S3 pricing model - Practice: Create S3 bucket and upload objects 23/09/2025 23/09/2025 https://000057.awsstudygroup.com/ Wed - Learn S3 advanced features: + Versioning + Lifecycle policies + Cross-Region Replication - Practice: Configure bucket versioning and lifecycle rules 24/09/2025 24/09/2025 https://000057.awsstudygroup.com/ Thu - Learn S3 Static Website Hosting: + Enable static hosting + Bucket policies for public access + Custom error pages - Practice: Deploy static website 25/09/2025 25/09/2025 https://000057.awsstudygroup.com/ Fri - Learn Amazon RDS basics: + Supported database engines + Multi-AZ deployments + Read Replicas - Practice: Launch RDS MySQL instance 26/09/2025 26/09/2025 https://000005.awsstudygroup.com/ Week 3 Achievements: Mastered AWS Cloud9 development environment:\nCreated and configured Cloud9 environment Used integrated terminal for AWS CLI commands Developed and tested code directly in the cloud Understood collaboration features for pair programming Understood Amazon S3 object storage:\nCreated S3 buckets with appropriate naming conventions Uploaded, downloaded, and managed objects Understood storage classes: Standard, IA, Glacier, etc. Configured bucket policies and ACLs Implemented S3 advanced features:\nEnabled versioning to protect against accidental deletion Created lifecycle policies for cost optimization Understood Cross-Region Replication for disaster recovery Successfully hosted static website on S3:\nEnabled static website hosting on bucket Configured bucket policy for public read access Set up index and error documents Deployed HTML/CSS/JS website Learned Amazon RDS database fundamentals:\nUnderstood managed database service benefits Launched RDS MySQL instance in VPC Configured security groups for database access Connected to RDS from EC2 instance Understood Multi-AZ for high availability "
},
{
	"uri": "http://localhost:1313/aws_ojt_report-TranThiThuHa/5-workshop/5.4-s3-onprem/",
	"title": "Access S3 from on-premises",
	"tags": [],
	"description": "",
	"content": "Overview In this section, you will create an Interface endpoint to access Amazon S3 from a simulated on-premises environment. The Interface endpoint will allow you to route to Amazon S3 over a VPN connection from your simulated on-premises environment.\nWhy using Interface endpoint:\nGateway endpoints only work with resources running in the VPC where they are created. Interface endpoints work with resources running in VPC, and also resources running in on-premises environments. Connectivty from your on-premises environment to the cloud can be provided by AWS Site-to-Site VPN or AWS Direct Connect. Interface endpoints allow you to connect to services powered by AWS PrivateLink. These services include some AWS services, services hosted by other AWS customers and partners in their own VPCs (referred to as PrivateLink Endpoint Services), and supported AWS Marketplace Partner services. For this workshop, we will focus on connecting to Amazon S3. "
},
{
	"uri": "http://localhost:1313/aws_ojt_report-TranThiThuHa/4-eventparticipated/",
	"title": "Events Participated",
	"tags": [],
	"description": "",
	"content": " In this section, you should list and describe in detail the events you have participated in during your internship or work experience.\nEach event should be presented in the format Event 1, Event 2, Event 3…, along with the following details:\nEvent name Date and time Location (if applicable) Your role in the event (attendee, event support, speaker, etc.) A brief description of the event’s content and main activities Outcomes or value gained (lessons learned, new skills, contribution to the team/project) This listing helps demonstrate your actual participation as well as the soft skills and experience you have gained from each event. During my internship, I participated in five events. Each one was a memorable experience that provided new, interesting, and useful knowledge, along with valuable networking opportunities and wonderful moments.\nEvent 1 Event Name: Vietnam Cloud Day 2025: Ho Chi Minh City Connect Edition for Builders\nDate \u0026amp; Time: Thursday, September 18, 2025, 9:00 AM - 5:00 PM\nLocation: Ho Chi Minh City, Vietnam\nRole: Attendee\nEvent 2 Event Name: AI-Driven Development Workshop – Shaping the Future of Development\nDate \u0026amp; Time: Friday, October 3, 2025, 14:00 – 16:30\nLocation: AWS Event Hall, 26th Floor – Bitexco Tower, Ho Chi Minh City\nRole: Attendee\nEvent 3 Event Name: AI/ML/GenAI on AWS Workshop\nDate \u0026amp; Time: Saturday, November 15, 2025, 8:30 AM – 12:00 PM\nLocation: AWS Vietnam Office\nRole: Attendee\nEvent 4 Event Name: DevOps on AWS Workshop\nDate \u0026amp; Time: Monday, November 17, 2025, 8:30 AM – 5:00 PM\nLocation: AWS Vietnam Office\nRole: Attendee\nEvent 5 Event Name: AWS Well-Architected Security Pillar Workshop\nDate \u0026amp; Time: Saturday, November 29, 2025, 8:30 AM – 12:00 PM\nLocation: AWS Vietnam Office\nRole: Attendee\n"
},
{
	"uri": "http://localhost:1313/aws_ojt_report-TranThiThuHa/5-workshop/5.4-s3-onprem/5.4.4-dns-simulation/",
	"title": "On-premises DNS Simulation",
	"tags": [],
	"description": "",
	"content": "AWS PrivateLink endpoints have a fixed IP address in each Availability Zone where they are deployed, for the life of the endpoint (until it is deleted). These IP addresses are attached to Elastic Network Interfaces (ENIs). AWS recommends using DNS to resolve the IP addresses for endpoints so that downstream applications use the latest IP addresses when ENIs are added to new AZs, or deleted over time.\nIn this section, you will create a forwarding rule to send DNS resolution requests from a simulated on-premises environment to a Route 53 Private Hosted Zone. This section leverages the infrastructure deployed by CloudFormation in the Prepare the environment section.\nCreate DNS Alias Records for the Interface endpoint Navigate to the Route 53 management console (Hosted Zones section). The CloudFormation template you deployed in the Prepare the environment section created this Private Hosted Zone. Click on the name of the Private Hosted Zone, s3.us-east-1.amazonaws.com: Create a new record in the Private Hosted Zone: Record name and record type keep default options Alias Button: Click to enable Route traffic to: Alias to VPC Endpoint Region: US East (N. Virginia) [us-east-1] Choose endpoint: Paste the Regional VPC Endpoint DNS name from your text editor (you saved when doing section 4.3) Click Add another record, and add a second record using the following values. Click Create records when finished to create both records. Record name: *. Record type: keep default value (type A) Alias Button: Click to enable Route traffic to: Alias to VPC Endpoint Region: US East (N. Virginia) [us-east-1] Choose endpoint: Paste the Regional VPC Endpoint DNS name from your text editor The new records appear in the Route 53 console:\nCreate a Resolver Forwarding Rule Route 53 Resolver Forwarding Rules allow you to forward DNS queries from your VPC to other sources for name resolution. Outside of a workshop environment, you might use this feature to forward DNS queries from your VPC to DNS servers running on-premises. In this section, you will simulate an on-premises conditional forwarder by creating a forwarding rule that forwards DNS queries for Amazon S3 to a Private Hosted Zone running in \u0026ldquo;VPC Cloud\u0026rdquo; in-order to resolve the PrivateLink interface endpoint regional DNS name.\nFrom the Route 53 management console, click Inbound endpoints on the left side bar In the Inbound endpoints console, click the ID of the inbound endpoint Copy the two IP addresses listed to your text editor From the Route 53 menu, choose Resolver \u0026gt; Rules, and click Create rule: In the Create rule console: Name: myS3Rule Rule type: Forward Domain name: s3.us-east-1.amazonaws.com VPC: VPC On-prem Outbound endpoint: VPCOnpremOutboundEndpoint Target IP Addresses: Enter both IP addresses from your text editor (inbound endpoint addresses) and then click Submit You have successfully created resolver forwarding rule.\nTest the on-premises DNS Simulation Connect to Test-Interface-Endpoint EC2 instance with Session manager Test DNS resolution. The dig command will return the IP addresses assigned to the VPC Interface endpoint running in VPC Cloud (your IP\u0026rsquo;s will be different): dig +short s3.us-east-1.amazonaws.com The IP addresses returned are the VPC endpoint IP addresses, NOT the Resolver IP addresses you pasted from your text editor. The IP addresses of the Resolver endpoint and the VPC endpoint look similar because they are all from the VPC Cloud CIDR block.\nNavigate to the VPC menu (Endpoints section), select the S3 Interface endpoint. Click the Subnets tab and verify that the IP addresses returned by Dig match the VPC endpoint: Return to your shell and use the AWS CLI to test listing your S3 buckets: aws s3 ls --endpoint-url https://s3.us-east-1.amazonaws.com Terminate your Session Manager session: In this section you created an Interface endpoint for Amazon S3. This endpoint can be reached from on-premises through Site-to-Site VPN or AWS Direct Connect. Route 53 Resolver outbound endpoints simulated forwarding DNS requests from on-premises to a Private Hosted Zone running the cloud. Route 53 inbound Endpoints recieved the resolution request and returned a response containing the IP addresses of the VPC interface endpoint. Using DNS to resolve the endpoint IP addresses provides high availability in-case of an Availability Zone outage.\n"
},
{
	"uri": "http://localhost:1313/aws_ojt_report-TranThiThuHa/1-worklog/1.4-week4/",
	"title": "Week 4 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 4 Objectives: Learn simplified computing with Amazon Lightsail. Understand application scaling with EC2 Auto Scaling. Master monitoring and observability with Amazon CloudWatch. Learn DNS management with Amazon Route 53. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material Mon - Learn Amazon Lightsail: + Lightsail vs EC2 + Instances, Blueprints + Networking \u0026amp; Storage - Practice: Deploy a WordPress site on Lightsail 29/09/2025 29/09/2025 https://000045.awsstudygroup.com/ Tue - Learn Lightsail Containers: + Container services + Deployment configurations + Public endpoints - Practice: Deploy containerized app on Lightsail 30/09/2025 30/09/2025 https://000046.awsstudygroup.com/ Wed - Learn EC2 Auto Scaling: + Launch Templates + Auto Scaling Groups + Scaling policies - Practice: Create Auto Scaling Group with scaling policies 01/10/2025 01/10/2025 https://000006.awsstudygroup.com/ Thu - Learn Amazon CloudWatch: + Metrics and Alarms + Logs and Dashboards + Events and Rules - Practice: Set up CloudWatch alarms for EC2 02/10/2025 02/10/2025 https://000008.awsstudygroup.com/ Fri - Learn Amazon Route 53: + DNS concepts + Hosted zones + Routing policies - Practice: Configure custom domain with Route 53 03/10/2025 03/10/2025 https://000010.awsstudygroup.com/ Week 4 Achievements: Mastered Amazon Lightsail for simplified computing:\nUnderstood differences between Lightsail and EC2 Created Lightsail instances with various blueprints Deployed WordPress website on Lightsail Configured static IP and DNS for Lightsail instance Learned Lightsail Containers:\nCreated container service on Lightsail Deployed containerized application Configured public endpoints and custom domains Understood container deployment workflow Implemented EC2 Auto Scaling:\nCreated Launch Templates for EC2 instances Configured Auto Scaling Groups (ASG) Set up scaling policies (target tracking, step scaling) Tested scaling based on CPU utilization Understood desired, minimum, and maximum capacity Mastered Amazon CloudWatch for monitoring:\nExplored default EC2 metrics (CPU, Network, Disk) Created custom CloudWatch alarms Set up alarm notifications via SNS Created CloudWatch dashboards for visualization Configured CloudWatch Logs for application logging Configured Amazon Route 53 for DNS:\nUnderstood DNS concepts (A, CNAME, Alias records) Created hosted zones for domains Configured different routing policies (Simple, Weighted, Latency) Integrated Route 53 with other AWS services (ALB, S3, CloudFront) "
},
{
	"uri": "http://localhost:1313/aws_ojt_report-TranThiThuHa/5-workshop/5.5-policy/",
	"title": "VPC Endpoint Policies",
	"tags": [],
	"description": "",
	"content": "When you create an interface or gateway endpoint, you can attach an endpoint policy to it that controls access to the service to which you are connecting. A VPC endpoint policy is an IAM resource policy that you attach to an endpoint. If you do not attach a policy when you create an endpoint, AWS attaches a default policy for you that allows full access to the service through the endpoint.\nYou can create a policy that restricts access to specific S3 buckets only. This is useful if you only want certain S3 Buckets to be accessible through the endpoint.\nIn this section you will create a VPC endpoint policy that restricts access to the S3 bucket specified in the VPC endpoint policy.\nConnect to an EC2 instance and verify connectivity to S3 Start a new AWS Session Manager session on the instance named Test-Gateway-Endpoint. From the session, verify that you can list the contents of the bucket you created in Part 1: Access S3 from VPC: aws s3 ls s3://\\\u0026lt;your-bucket-name\\\u0026gt; The bucket contents include the two 1 GB files uploaded in earlier.\nCreate a new S3 bucket; follow the naming pattern you used in Part 1, but add a \u0026lsquo;-2\u0026rsquo; to the name. Leave other fields as default and click create Successfully create bucket\nNavigate to: Services \u0026gt; VPC \u0026gt; Endpoints, then select the Gateway VPC endpoint you created earlier. Click the Policy tab. Click Edit policy. The default policy allows access to all S3 Buckets through the VPC endpoint.\nIn Edit Policy console, copy \u0026amp; Paste the following policy, then replace yourbucketname-2 with your 2nd bucket name. This policy will allow access through the VPC endpoint to your new bucket, but not any other bucket in Amazon S3. Click Save to apply the policy. { \u0026#34;Id\u0026#34;: \u0026#34;Policy1631305502445\u0026#34;, \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;Stmt1631305501021\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;s3:*\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::yourbucketname-2\u0026#34;, \u0026#34;arn:aws:s3:::yourbucketname-2/*\u0026#34; ], \u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34; } ] } Successfully customize policy\nFrom your session on the Test-Gateway-Endpoint instance, test access to the S3 bucket you created in Part 1: Access S3 from VPC aws s3 ls s3://\u0026lt;yourbucketname\u0026gt; This command will return an error because access to this bucket is not permitted by your new VPC endpoint policy:\nReturn to your home directory on your EC2 instance cd~ Create a file fallocate -l 1G test-bucket2.xyz Copy file to 2nd bucket aws s3 cp test-bucket2.xyz s3://\u0026lt;your-2nd-bucket-name\u0026gt; This operation succeeds because it is permitted by the VPC endpoint policy.\nThen we test access to the first bucket by copy the file to 1st bucket aws s3 cp test-bucket2.xyz s3://\u0026lt;your-1st-bucket-name\u0026gt; This command will return an error because access to this bucket is not permitted by your new VPC endpoint policy.\nPart 3 Summary: In this section, you created a VPC endpoint policy for Amazon S3, and used the AWS CLI to test the policy. AWS CLI actions targeted to your original S3 bucket failed because you applied a policy that only allowed access to the second bucket you created. AWS CLI actions targeted for your second bucket succeeded because the policy allowed them. These policies can be useful in situations where you need to control access to resources through VPC endpoints.\n"
},
{
	"uri": "http://localhost:1313/aws_ojt_report-TranThiThuHa/1-worklog/1.5-week5/",
	"title": "Week 5 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 5 Objectives: Learn NoSQL database with Amazon DynamoDB. Understand in-memory caching with Amazon ElastiCache. Master content delivery with Amazon CloudFront. Learn edge computing with CloudFront and Lambda@Edge. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material Mon - Learn Amazon DynamoDB basics: + Tables, Items, Attributes + Primary Keys (Partition, Sort) + Read/Write Capacity - Practice: Create DynamoDB table 06/10/2025 06/10/2025 https://000060.awsstudygroup.com/ Tue - Learn DynamoDB advanced features: + Secondary Indexes (GSI, LSI) + DynamoDB Streams + TTL - Practice: Query and Scan operations 07/10/2025 07/10/2025 https://000060.awsstudygroup.com/ Wed - Learn Amazon ElastiCache: + Redis vs Memcached + Cluster configurations + Caching strategies - Practice: Create ElastiCache Redis cluster 08/10/2025 08/10/2025 https://000061.awsstudygroup.com/ Thu - Learn Amazon CloudFront: + CDN concepts + Distributions, Origins + Cache behaviors - Practice: Create CloudFront distribution for S3 09/10/2025 09/10/2025 https://000094.awsstudygroup.com/ Fri - Learn CloudFront \u0026amp; Lambda@Edge: + Edge functions + Use cases (URL rewrite, auth) + Deployment - Practice: Implement Lambda@Edge function 10/10/2025 10/10/2025 https://000095.awsstudygroup.com/ Week 5 Achievements: Mastered Amazon DynamoDB NoSQL database:\nCreated DynamoDB tables with partition and sort keys Understood provisioned vs on-demand capacity modes Performed CRUD operations using AWS Console and CLI Implemented Global Secondary Indexes (GSI) for flexible queries Configured DynamoDB Streams for event-driven architectures Set up TTL for automatic data expiration Learned Amazon ElastiCache for in-memory caching:\nUnderstood differences between Redis and Memcached Created ElastiCache Redis cluster in VPC Configured security groups for cache access Implemented caching strategies (lazy loading, write-through) Connected application to ElastiCache from EC2 Implemented Amazon CloudFront CDN:\nCreated CloudFront distribution with S3 origin Configured cache behaviors and TTL settings Set up custom domain with SSL certificate (ACM) Implemented Origin Access Control (OAC) for S3 Understood edge locations and regional caches Explored CloudFront with Lambda@Edge:\nUnderstood edge computing concepts Created Lambda@Edge functions for request/response manipulation Implemented URL rewrites and redirects Configured viewer request and origin request triggers Deployed edge functions globally "
},
{
	"uri": "http://localhost:1313/aws_ojt_report-TranThiThuHa/5-workshop/",
	"title": "Workshop",
	"tags": [],
	"description": "",
	"content": " ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nSecure Hybrid Access to S3 using VPC Endpoints Overview AWS PrivateLink provides private connectivity to AWS services from VPCs and your on-premises networks, without exposing your traffic to the Public Internet.\nIn this lab, you will learn how to create, configure, and test VPC endpoints that enable your workloads to reach AWS services without traversing the Public Internet.\nYou will create two types of endpoints to access Amazon S3: a Gateway VPC endpoint, and an Interface VPC endpoint. These two types of VPC endpoints offer different benefits depending on if you are accessing Amazon S3 from the cloud or your on-premises location\nGateway - Create a gateway endpoint to send traffic to Amazon S3 or DynamoDB using private IP addresses.You route traffic from your VPC to the gateway endpoint using route tables. Interface - Create an interface endpoint to send traffic to endpoint services that use a Network Load Balancer to distribute traffic. Traffic destined for the endpoint service is resolved using DNS. Content Workshop overview Prerequiste Access S3 from VPC Access S3 from On-premises VPC Endpoint Policies (Bonus) Clean up "
},
{
	"uri": "http://localhost:1313/aws_ojt_report-TranThiThuHa/5-workshop/5.6-cleanup/",
	"title": "Clean up",
	"tags": [],
	"description": "",
	"content": "Congratulations on completing this workshop! In this workshop, you learned architecture patterns for accessing Amazon S3 without using the Public Internet.\nBy creating a gateway endpoint, you enabled direct communication between EC2 resources and Amazon S3, without traversing an Internet Gateway. By creating an interface endpoint you extended S3 connectivity to resources running in your on-premises data center via AWS Site-to-Site VPN or Direct Connect. clean up Navigate to Hosted Zones on the left side of Route 53 console. Click the name of s3.us-east-1.amazonaws.com zone. Click Delete and confirm deletion by typing delete. Disassociate the Route 53 Resolver Rule - myS3Rule from \u0026ldquo;VPC Onprem\u0026rdquo; and Delete it. Open the CloudFormation console and delete the two CloudFormation Stacks that you created for this lab: PLOnpremSetup PLCloudSetup Delete S3 buckets Open S3 console Choose the bucket we created for the lab, click and confirm empty. Click delete and confirm delete. "
},
{
	"uri": "http://localhost:1313/aws_ojt_report-TranThiThuHa/6-self-evaluation/",
	"title": "Self-Assessment",
	"tags": [],
	"description": "",
	"content": "During my internship at First Cloud Journey (FCJ) from September 8, 2025 to November 28, 2025, I had the opportunity to learn, practice, and apply the knowledge acquired in school to a real-world working environment.\nI participated in the AWS Cloud Engineer internship program following the 12-week learning path at cloudjourney.awsstudygroup.com, through which I improved my skills in AWS cloud services, Infrastructure as Code (CloudFormation, CDK), monitoring and logging (CloudWatch, Grafana), security best practices (IAM, WAF, KMS, GuardDuty), cost optimization, and CI/CD pipelines.\nIn terms of work ethic, I always strived to complete tasks well, complied with workplace regulations, and actively engaged with colleagues to improve work efficiency. Throughout the 12 weeks, I systematically explored AWS services (Weeks 1-5), optimized operations (Weeks 6-8), enhanced security practices (Weeks 9-11), and completed a comprehensive final project building a complete AWS architecture (Week 12).\nTo objectively reflect on my internship period, I would like to evaluate myself based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge in practice, proficiency with tools, work quality ✅ ☐ ☐ 2 Ability to learn Ability to absorb new knowledge and learn quickly ☐ ✅ ☐ 3 Proactiveness Taking initiative, seeking out tasks without waiting for instructions ✅ ☐ ☐ 4 Sense of responsibility Completing tasks on time and ensuring quality ✅ ☐ ☐ 5 Discipline Adhering to schedules, rules, and work processes ☐ ☐ ✅ 6 Progressive mindset Willingness to receive feedback and improve oneself ☐ ✅ ☐ 7 Communication Presenting ideas and reporting work clearly ☐ ✅ ☐ 8 Teamwork Working effectively with colleagues and participating in teams ✅ ☐ ☐ 9 Professional conduct Respecting colleagues, partners, and the work environment ✅ ☐ ☐ 10 Problem-solving skills Identifying problems, proposing solutions, and showing creativity ☐ ✅ ☐ 11 Contribution to project/team Work effectiveness, innovative ideas, recognition from the team ✅ ☐ ☐ 12 Overall General evaluation of the entire internship period ✅ ☐ ☐ Needs Improvement Strengthen discipline and strictly comply with schedules, rules, and work processes to ensure consistent productivity Improve problem-solving thinking, especially when troubleshooting complex AWS infrastructure issues and debugging CloudFormation/CDK deployments Enhance communication skills in both daily interactions and professional contexts, including effectively presenting technical solutions, documenting architecture decisions, and handling unexpected situations during deployments "
},
{
	"uri": "http://localhost:1313/aws_ojt_report-TranThiThuHa/1-worklog/1.6-week6/",
	"title": "Week 6 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 6 Objectives: Learn serverless computing with AWS Lambda. Master advanced monitoring with CloudWatch and Grafana. Understand resource organization with Tags and Resource Groups. Learn systems management with AWS Systems Manager. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material Mon - Learn AWS Lambda basics: + Serverless concepts + Function creation + Triggers and events - Practice: Create Lambda function with API Gateway 13/10/2025 13/10/2025 https://000022.awsstudygroup.com/ Tue - Learn Lambda advanced: + Environment variables + Layers and versions + Concurrency - Practice: Lambda with S3 and DynamoDB triggers 14/10/2025 14/10/2025 https://000022.awsstudygroup.com/ Wed - Learn Advanced Monitoring: + CloudWatch advanced metrics + Grafana integration + Custom dashboards - Practice: Set up Grafana with CloudWatch 15/10/2025 15/10/2025 https://000029.awsstudygroup.com/ Thu - Learn Tags and Resource Groups: + Tagging strategies + Resource Groups + Tag-based access control - Practice: Implement tagging strategy 16/10/2025 16/10/2025 https://000027.awsstudygroup.com/ Fri - Learn AWS Systems Manager: + Session Manager + Parameter Store + Run Command - Practice: Manage EC2 with Systems Manager 17/10/2025 17/10/2025 https://000031.awsstudygroup.com/ Week 6 Achievements: Mastered AWS Lambda serverless computing:\nCreated Lambda functions using Python/Node.js Configured API Gateway as Lambda trigger Set up S3 and DynamoDB event triggers Managed environment variables and secrets Understood Lambda layers for code reuse Configured concurrency and memory settings Learned advanced monitoring with CloudWatch and Grafana:\nCreated custom CloudWatch metrics Set up Grafana for advanced visualization Built custom dashboards for monitoring Configured alerts and notifications Implemented log insights queries Implemented Tags and Resource Groups:\nDesigned tagging strategy for organization Created Resource Groups for management Implemented tag-based access control with IAM Used tags for cost allocation Automated tagging with AWS Config Mastered AWS Systems Manager:\nConnected to EC2 without SSH using Session Manager Stored secrets in Parameter Store Executed commands on multiple instances with Run Command Set up Patch Manager for automated patching Created automation documents for routine tasks "
},
{
	"uri": "http://localhost:1313/aws_ojt_report-TranThiThuHa/7-feedback/",
	"title": "Sharing and Feedback",
	"tags": [],
	"description": "",
	"content": " This section provides my personal reflections and feedback on my experience participating in the First Cloud Journey (FCJ) AWS Cloud Engineer Internship Program. I hope these insights will help the FCJ team continuously improve the program and provide even better experiences for future interns.\nOverall Evaluation 1. Working Environment The working environment at FCJ is exceptionally supportive and conducive to learning. From the very first week, I felt welcomed and integrated into the team. The remote/hybrid working model provided flexibility while maintaining clear communication channels through regular check-ins and collaborative platforms.\nWhat impressed me most was the culture of knowledge sharing - team members were always willing to help, whether it was troubleshooting a CloudFormation deployment issue or explaining complex AWS service architectures. The structured 12-week learning path at cloudjourney.awsstudygroup.com provided clear direction while allowing room for exploration and hands-on practice.\nSuggestion: Consider organizing more virtual networking sessions or technical workshops where interns can interact with senior engineers and learn about real-world cloud projects.\n2. Support from Mentor / Team Admin The mentorship I received was outstanding and truly transformative. My mentor didn\u0026rsquo;t just provide answers but guided me through problem-solving processes, which significantly enhanced my technical thinking and troubleshooting abilities.\nKey highlights:\nStructured guidance through the 12-week curriculum covering AWS fundamentals, operations optimization, and security best practices Encouragement of independent learning - I was encouraged to explore AWS documentation, experiment with services, and learn from mistakes Timely feedback on my worklog entries and project implementations Real-world context - explanations of how concepts apply in production environments The admin team was equally supportive, ensuring all necessary resources, AWS account access, and documentation were readily available. The onboarding process was smooth and well-organized.\nSuggestion: Perhaps introduce a peer mentoring system where previous interns can share their experiences with new cohorts.\n3. Relevance of Work to Academic Major The internship program was highly relevant to my Information Technology major and perfectly bridged the gap between academic knowledge and industry practice.\nAlignment with academic learning:\nApplied networking concepts (VPC, subnets, security groups) learned in university Practiced database management with RDS and DynamoDB Implemented security principles through IAM, WAF, and encryption services Applied software engineering practices with Infrastructure as Code (CloudFormation, CDK) New areas explored:\nCloud-native architectures and serverless computing (Lambda, API Gateway) DevOps practices (CI/CD pipelines, monitoring with CloudWatch) Cost optimization strategies and resource right-sizing Enterprise security frameworks (GuardDuty, Security Hub, Macie) The progression from basic AWS services (Weeks 1-5) to operations optimization (Weeks 6-8) and security (Weeks 9-11) was well-designed and built upon previous knowledge systematically.\n4. Learning \u0026amp; Skill Development Opportunities This internship provided comprehensive skill development across technical, professional, and soft skills.\nTechnical skills acquired:\nAWS Services Mastery: Hands-on experience with 30+ AWS services including EC2, S3, RDS, DynamoDB, Lambda, CloudFront, VPC, IAM, CloudWatch, and more Infrastructure as Code: Proficient in CloudFormation (YAML/JSON) and AWS CDK (TypeScript) Monitoring \u0026amp; Observability: CloudWatch metrics, alarms, dashboards, and integration with Grafana Security Best Practices: IAM policies, WAF rules, KMS encryption, Secrets Manager, GuardDuty Cost Management: AWS Budgets, Cost Explorer, Compute Optimizer, right-sizing strategies Professional skills developed:\nTechnical documentation: Writing detailed worklogs and architecture documentation Problem-solving: Troubleshooting deployment issues, debugging CloudFormation templates, resolving security misconfigurations Project management: Managing a 12-week learning journey with weekly milestones Professional communication: Presenting technical solutions and architecture decisions Career impact: The internship significantly enhanced my understanding of cloud computing and prepared me for AWS certifications. The final project (Week 12) where I designed and deployed a complete AWS architecture was particularly valuable, as it demonstrated my ability to integrate multiple services into a cohesive solution.\n5. Company Culture \u0026amp; Team Spirit FCJ\u0026rsquo;s culture is collaborative, inclusive, and growth-oriented. Despite being an intern, I felt valued as a team member. The culture emphasizes:\nContinuous learning: Everyone is encouraged to explore new technologies and share knowledge Mutual respect: Open communication regardless of seniority Ownership mindset: Taking responsibility for learning outcomes and project deliverables Innovation: Encouragement to experiment and learn from failures The team\u0026rsquo;s response to questions and challenges was always constructive and supportive. When I encountered issues with complex deployments or security configurations, team members provided guidance without making me feel inadequate.\nSuggestion: Consider creating a knowledge base or wiki where interns can contribute their learnings and solutions for future reference.\n6. Internship Policies / Benefits The internship program offers competitive benefits that support both learning and professional development:\nStructured learning path: Clear 12-week curriculum with defined objectives and outcomes Hands-on practice: Access to AWS Free Tier and necessary resources for experimentation Flexible schedule: Accommodation for academic commitments Professional development: Opportunity to work on real-world projects and build a portfolio Networking opportunities: Interaction with industry professionals and fellow interns The program structure allowed me to balance learning new concepts with practical application, which maximized knowledge retention and skill development.\nAdditional Questions What did you find most satisfying during your internship? The most satisfying aspect was witnessing my growth from a beginner with basic AWS knowledge to someone capable of designing and deploying complete cloud architectures. Specifically:\nThe \u0026ldquo;aha\u0026rdquo; moments when complex concepts clicked - understanding how VPC networking works, grasping IAM permission boundaries, or successfully debugging a CloudFormation stack Completing the final project - building an end-to-end AWS solution that integrated multiple services (VPC, EC2, S3, RDS, Lambda, CloudFront, CloudWatch) was incredibly rewarding Problem-solving victories - successfully troubleshooting issues like CloudFormation drift detection, configuring WAF rules, or optimizing EC2 instance sizes Knowledge application - seeing how theoretical concepts from university applied to real-world cloud infrastructure What do you think the company should improve for future interns? Suggestions for improvement:\nMore interactive sessions: Regular Q\u0026amp;A sessions or office hours with mentors to discuss challenges and advanced topics Peer collaboration: More opportunities for interns to collaborate on projects or share learnings with each other Advanced track option: For interns who progress quickly, an optional advanced track covering more complex topics (multi-region architectures, advanced security, compliance frameworks) Portfolio development: Guidance on how to showcase internship projects in portfolios and resumes Certification support: Information sessions or study groups for AWS certifications (Solutions Architect, Developer, SysOps) If recommending to a friend, would you suggest they intern here? Why or why not? Absolutely, I would highly recommend this internship to anyone interested in cloud computing and AWS. Here\u0026rsquo;s why:\nStrengths:\nComprehensive learning: The 12-week structured path covers everything from fundamentals to advanced topics Hands-on experience: Real AWS account access and practical projects, not just theoretical learning Supportive environment: Excellent mentorship and team support Industry relevance: Skills directly applicable to cloud engineering roles Portfolio building: Opportunity to create impressive projects for job applications Ideal for:\nStudents interested in cloud computing and DevOps Those preparing for AWS certifications Anyone wanting to transition into cloud engineering careers Students seeking practical, industry-relevant experience The program provides exceptional value and sets a strong foundation for a career in cloud computing.\nSuggestions \u0026amp; Expectations Do you have any suggestions to improve the internship experience? Technical improvements:\nLab environments: Pre-configured lab environments for complex scenarios (multi-account setups, disaster recovery exercises) Code review sessions: Regular code reviews of CloudFormation/CDK templates to learn best practices Architecture reviews: Group sessions where interns present their final projects and receive feedback Troubleshooting workshops: Dedicated sessions on common issues and debugging techniques Program structure:\nMilestone celebrations: Recognition for completing major phases (completing weeks 1-5, 6-8, 9-11) Guest speakers: Sessions with AWS solutions architects or senior cloud engineers Case studies: Analysis of real-world AWS architectures and design decisions Alumni network: Connection with previous interns for networking and career advice Would you like to continue this program in the future? Yes, I would be interested in:\nAdvanced internship track: If available, I would love to continue with more advanced topics Mentorship role: Contributing as a peer mentor for future interns Contributing to curriculum: Helping improve learning materials based on my experience Staying connected: Remaining part of the FCJ community for networking and knowledge sharing Any other comments (free sharing): This internship has been a transformative experience that exceeded my expectations. The combination of structured learning, hands-on practice, and supportive mentorship created an ideal environment for growth.\nKey takeaways:\nCloud computing is not just about knowing services, but understanding how to architect solutions that are secure, scalable, and cost-effective Infrastructure as Code is essential for modern cloud engineering Security should be built into architectures from the start, not added as an afterthought Continuous learning and staying updated with AWS services is crucial in this field Gratitude: I\u0026rsquo;m deeply grateful to the FCJ team for this opportunity. The knowledge and skills I\u0026rsquo;ve gained will be invaluable in my career. The program has not only taught me AWS but also instilled a mindset of continuous learning and professional growth.\nI hope this feedback helps improve the program for future interns. Thank you for an incredible 12 weeks of learning and growth!\n\u0026ldquo;The best way to learn cloud computing is by doing, and FCJ provided the perfect environment for that.\u0026rdquo;\n"
},
{
	"uri": "http://localhost:1313/aws_ojt_report-TranThiThuHa/1-worklog/1.7-week7/",
	"title": "Week 7 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 7 Objectives: Learn Infrastructure as Code with AWS CloudFormation. Master AWS CDK basics and advanced features. Understand EC2 resource optimization and right-sizing. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material Mon - Learn AWS CloudFormation: + Templates (YAML/JSON) + Stacks and StackSets + Intrinsic functions - Practice: Create VPC stack with CloudFormation 20/10/2025 20/10/2025 https://000037.awsstudygroup.com/ Tue - Learn CloudFormation advanced: + Nested stacks + Change sets + Drift detection - Practice: Deploy multi-tier application with CloudFormation 21/10/2025 21/10/2025 https://000037.awsstudygroup.com/ Wed - Learn AWS CDK: + CDK concepts and constructs + CDK vs CloudFormation + Supported languages - Practice: Create infrastructure with CDK TypeScript 22/10/2025 22/10/2025 https://000038.awsstudygroup.com/ Thu - Learn AWS CDK Advanced: + Custom constructs + CDK Pipelines + Testing CDK apps - Practice: Build CI/CD pipeline with CDK Pipelines 23/10/2025 23/10/2025 https://000076.awsstudygroup.com/ Fri - Learn EC2 Right-Sizing: + Resource optimization concepts + AWS Compute Optimizer + Cost savings strategies - Practice: Analyze and right-size EC2 instances 24/10/2025 24/10/2025 https://000032.awsstudygroup.com/ Week 7 Achievements: Mastered AWS CloudFormation for Infrastructure as Code:\nCreated CloudFormation templates in YAML/JSON Deployed and managed stacks for VPC, EC2, RDS Used intrinsic functions (Ref, Fn::GetAtt, Fn::Join) Implemented nested stacks for modularity Used change sets for safe updates Detected and remediated configuration drift Learned AWS CDK for cloud development:\nUnderstood CDK constructs (L1, L2, L3) Created infrastructure using TypeScript/Python Synthesized CloudFormation templates from CDK Deployed stacks with cdk deploy Used CDK patterns for best practices Mastered AWS CDK Advanced features:\nCreated custom L2 and L3 constructs Built CI/CD pipelines with CDK Pipelines Implemented unit tests for CDK apps Used CDK aspects for compliance Deployed multi-stage applications Implemented EC2 Right-Sizing for cost optimization:\nUsed AWS Compute Optimizer for recommendations Analyzed EC2 instance utilization metrics Identified over-provisioned and under-utilized instances Resized instances to match workload requirements Achieved cost savings through right-sizing "
},
{
	"uri": "http://localhost:1313/aws_ojt_report-TranThiThuHa/1-worklog/1.8-week8/",
	"title": "Week 8 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 8 Objectives: Learn remote server access with Systems Manager Session Manager. Master network monitoring with VPC Flow Logs. Understand service quotas management. Learn cost and usage management on AWS. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material Mon - Learn Session Manager: + Secure shell access without SSH + IAM policies for Session Manager + Logging and auditing - Practice: Connect to EC2 via Session Manager 27/10/2025 27/10/2025 https://000058.awsstudygroup.com/ Tue - Learn VPC Flow Logs: + Flow log concepts + Log destinations (S3, CloudWatch) + Analyzing network traffic - Practice: Enable and analyze VPC Flow Logs 28/10/2025 28/10/2025 https://000074.awsstudygroup.com/ Wed - Learn Service Quotas: + Default vs applied quotas + Requesting quota increases + Quota alarms - Practice: View and request quota increases 29/10/2025 29/10/2025 https://000063.awsstudygroup.com/ Thu - Learn Cost and Usage Management: + Resource usage with IAM + Cost allocation tags + Usage restrictions - Practice: Implement cost management with IAM 30/10/2025 30/10/2025 https://000064.awsstudygroup.com/ Fri - Learn EBS Data Lifecycle Manager: + Snapshot lifecycle policies + Automated archival + Retention policies - Practice: Automate EBS snapshot archival 31/10/2025 31/10/2025 https://000088.awsstudygroup.com/ Week 8 Achievements: Mastered Systems Manager Session Manager:\nConnected to EC2 instances without SSH keys or bastion hosts Configured IAM policies for Session Manager access Enabled session logging to S3 and CloudWatch Used Session Manager for port forwarding Understood security benefits over traditional SSH Implemented VPC Flow Logs for network monitoring:\nEnabled VPC Flow Logs at VPC, subnet, and ENI levels Configured log destinations (CloudWatch Logs, S3) Analyzed network traffic patterns Identified security issues and troubleshot connectivity Created CloudWatch metrics from flow logs Managed Service Quotas effectively:\nUnderstood default and applied quotas Viewed quotas across AWS services Requested quota increases when needed Set up CloudWatch alarms for quota monitoring Planned capacity based on quota limits Implemented Resource Usage and Cost Management with IAM:\nCreated IAM policies to limit resource usage by region Restricted EC2 instance families and sizes Limited EBS volume types for cost control Implemented least privilege for cost management Validated policy effectiveness Automated EBS Snapshots with Data Lifecycle Manager:\nCreated snapshot lifecycle policies Configured automated snapshot archival Set up retention rules for compliance Used single and multiple policy schedules Achieved cost savings with snapshot archive tier "
},
{
	"uri": "http://localhost:1313/aws_ojt_report-TranThiThuHa/1-worklog/1.9-week9/",
	"title": "Week 9 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 9 Objectives: Learn identity federation with AWS Single Sign-On. Understand IAM Permission Boundaries. Master access control with IAM Policies and Conditions. Learn security compliance with AWS Security Hub. Understand private connectivity with VPC Endpoints. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material Mon - Learn AWS Single Sign-On: + Identity federation + SSO for Organizations + Permission sets - Practice: Set up SSO for AWS Organization 03/11/2025 03/11/2025 https://000012.awsstudygroup.com/ Tue - Learn IAM Permission Boundaries: + Boundary concepts + Delegating permissions + Use cases - Practice: Limit user permissions with boundaries 04/11/2025 04/11/2025 https://000030.awsstudygroup.com/ Wed - Learn IAM Policies and Conditions: + Policy conditions + Role switching limits + Context keys - Practice: Implement conditional role switching 05/11/2025 05/11/2025 https://000044.awsstudygroup.com/ Thu - Learn AWS Security Hub: + Security standards + Findings aggregation + Compliance checks - Practice: Enable Security Hub and review compliance 06/11/2025 06/11/2025 https://000018.awsstudygroup.com/ Fri - Learn VPC Endpoints: + Gateway vs Interface endpoints + Private S3 access + Endpoint policies - Practice: Set up private S3 access via VPC Endpoint 07/11/2025 07/11/2025 https://000111.awsstudygroup.com/ Week 9 Achievements: Implemented AWS Single Sign-On:\nSet up SSO for AWS Organizations Configured identity sources (AWS SSO, AD, external IdP) Created permission sets for different roles Enabled SSO access to multiple AWS accounts Understood federation and SAML integration Mastered IAM Permission Boundaries:\nUnderstood permission boundary concepts Created boundaries to limit maximum permissions Delegated user creation safely to developers Prevented privilege escalation Combined with service control policies Implemented IAM Policies and Conditions:\nUsed condition keys in IAM policies Implemented time-based access restrictions Limited role switching with conditions Used resource tags for access control Created fine-grained access policies Configured AWS Security Hub for compliance:\nEnabled AWS Foundational Security Best Practices Reviewed CIS AWS Foundations Benchmark Aggregated findings from GuardDuty, Inspector, Macie Set up automated remediation workflows Created custom security insights Configured VPC Endpoints for private access:\nCreated Gateway Endpoint for S3 Created Interface Endpoints for other AWS services Configured endpoint policies for access control Enabled private DNS for seamless integration Eliminated need for NAT Gateway for AWS services "
},
{
	"uri": "http://localhost:1313/aws_ojt_report-TranThiThuHa/1-worklog/1.10-week10/",
	"title": "Week 10 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 10 Objectives: Learn application protection with AWS WAF. Understand encryption with AWS KMS. Learn data protection with Amazon Macie. Master credentials management with AWS Secrets Manager. Understand security governance with AWS Firewall Manager. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material Mon - Learn AWS WAF: + Web ACLs and rules + Managed rule groups + Rate-based rules - Practice: Protect ALB/CloudFront with WAF 10/11/2025 10/11/2025 https://000026.awsstudygroup.com/ Tue - Learn AWS KMS: + Customer managed keys + Key policies + Encryption context - Practice: Encrypt S3 and EBS with KMS 11/11/2025 11/11/2025 https://000033.awsstudygroup.com/ Wed - Learn Amazon Macie: + Data discovery + Sensitive data detection + Findings and alerts - Practice: Scan S3 buckets for sensitive data 12/11/2025 12/11/2025 https://000090.awsstudygroup.com/ Thu - Learn AWS Secrets Manager: + Secret storage + Automatic rotation + Cross-account access - Practice: Store and rotate database credentials 13/11/2025 13/11/2025 https://000096.awsstudygroup.com/ Fri - Learn AWS Firewall Manager: + Security policies + Cross-account management + WAF rules deployment - Practice: Deploy WAF rules across accounts 14/11/2025 14/11/2025 https://000097.awsstudygroup.com/ Week 10 Achievements: Implemented AWS WAF for application protection:\nCreated Web ACLs with custom and managed rules Used AWS Managed Rules for OWASP Top 10 Configured rate-based rules for DDoS mitigation Associated WAF with ALB and CloudFront Monitored WAF metrics and sampled requests Mastered AWS KMS for encryption:\nCreated customer managed keys (CMK) Configured key policies and IAM policies Encrypted S3 buckets with SSE-KMS Encrypted EBS volumes with KMS keys Understood key rotation and auditing Implemented Amazon Macie for data protection:\nEnabled Macie for S3 buckets Configured sensitive data discovery jobs Analyzed findings for PII and financial data Set up alerts for sensitive data exposure Created custom data identifiers Configured AWS Secrets Manager:\nStored database credentials securely Configured automatic secret rotation Retrieved secrets from Lambda functions Implemented cross-account secret access Integrated with RDS for automatic rotation Implemented AWS Firewall Manager:\nCreated security policies for WAF Deployed rules across multiple accounts Managed Security Groups centrally Configured Shield Advanced policies Monitored compliance across organization "
},
{
	"uri": "http://localhost:1313/aws_ojt_report-TranThiThuHa/1-worklog/1.11-week11/",
	"title": "Week 11 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 11 Objectives: Learn threat detection with AWS GuardDuty. Understand automated patching with EC2 Image Builder. Master authentication with Amazon Cognito. Learn S3 Security Best Practices. Begin Reliability section with AWS Backup. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material Mon - Learn AWS GuardDuty: + Threat detection + Finding types + Integration with EventBridge - Practice: Enable GuardDuty and analyze findings 17/11/2025 17/11/2025 https://000098.awsstudygroup.com/ Tue - Learn EC2 Image Builder: + Image pipelines + Components and recipes + Automated patching - Practice: Create automated AMI pipeline 18/11/2025 18/11/2025 https://000099.awsstudygroup.com/ Wed - Learn Amazon Cognito: + User pools + Identity pools + Social identity providers - Practice: Implement authentication for web app 19/11/2025 19/11/2025 https://000141.awsstudygroup.com/ Thu - Learn S3 Security Best Practices: + Bucket policies + Access control lists + Encryption settings - Practice: Secure S3 buckets 20/11/2025 20/11/2025 https://000069.awsstudygroup.com/ Fri - Learn AWS Backup (Reliability): + Backup plans + Backup vaults + Cross-region backup - Practice: Create backup plan for EC2 and RDS 21/11/2025 21/11/2025 https://000013.awsstudygroup.com/ Week 11 Achievements: Implemented AWS GuardDuty for threat detection:\nEnabled GuardDuty in multiple regions Analyzed different finding types (EC2, IAM, S3) Configured EventBridge rules for alerts Integrated with SNS for notifications Understood threat intelligence sources Mastered EC2 Image Builder for automated patching:\nCreated image pipelines for AMI automation Built components and recipes for configurations Scheduled automated image builds Implemented security hardening in images Distributed AMIs across regions Implemented Amazon Cognito for authentication:\nCreated user pools for user management Configured identity pools for AWS access Integrated social identity providers (Google, Facebook) Implemented MFA for enhanced security Set up hosted UI for authentication flows Applied S3 Security Best Practices:\nConfigured bucket policies for access control Enabled server-side encryption (SSE-S3, SSE-KMS) Blocked public access at account level Set up S3 Access Analyzer for monitoring Implemented versioning and MFA delete Started Reliability section with AWS Backup:\nCreated backup plans with retention policies Set up backup vaults with encryption Configured cross-region backup for DR Implemented backup for EC2, RDS, EFS Set up lifecycle policies for cost optimization "
},
{
	"uri": "http://localhost:1313/aws_ojt_report-TranThiThuHa/1-worklog/1.12-week12/",
	"title": "Week 12 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 12 Objectives: Review and consolidate all AWS services learned throughout the internship. Design and implement a final project combining multiple AWS services. Complete the learning roadmap and prepare for real-world applications. Write final report and present the project. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material Mon - Review all core services: EC2, S3, RDS, DynamoDB, IAM, VPC, Lambda, CloudWatch, CloudFront, API Gateway - Define requirements and architecture for the final project 24/11/2025 24/11/2025 https://cloudjourney.awsstudygroup.com/ Tue - Start project deployment: + Design VPC, subnet, security groups + Configure S3, CloudFront, RDS/DynamoDB (depending on project) 25/11/2025 25/11/2025 https://cloudjourney.awsstudygroup.com/ Wed - Continue project implementation: + Build backend using Lambda / API Gateway or EC2 (depending on architecture) + Connect database and process data + Integrate CloudWatch for monitoring 26/11/2025 26/11/2025 https://cloudjourney.awsstudygroup.com/ Thu - Complete the project: + Add Cognito authentication if needed + Finalize CI/CD pipeline (CodePipeline/CodeBuild) + End-to-end system testing 27/11/2025 27/11/2025 https://cloudjourney.awsstudygroup.com/ Fri - Write final report - Prepare presentation (architecture, service selection rationale, cost, security) - Summarize the entire learning journey and self-evaluate capabilities 28/11/2025 28/11/2025 https://cloudjourney.awsstudygroup.com/ Week 12 Achievements: Completed the final AWS project combining multiple important services:\nDesigned scalable architecture with VPC, subnets, and security groups Implemented storage solutions with S3 and CloudFront CDN Built database layer with RDS or DynamoDB Created serverless backend with Lambda and API Gateway Integrated monitoring with CloudWatch Self-designed architecture, deployed, optimized, and operated a real-world system:\nApplied best practices for security (IAM, Security Groups, encryption) Implemented high availability and fault tolerance Configured auto-scaling for cost optimization Set up CI/CD pipeline for automated deployments Mastered the process of building Cloud applications from start to finish:\nRequirements analysis and architecture design Infrastructure provisioning and configuration Application deployment and testing Monitoring, logging, and troubleshooting Documentation and presentation Completed the 12-week AWS learning roadmap:\nWeek 1-5: Explored AWS Services (Compute, Storage, Networking, Database) Week 6-8: Optimizing Operations (Lambda, CloudFormation, CDK, Systems Manager) Week 9-11: Optimizing Security (IAM, WAF, KMS, GuardDuty, Cognito) Week 12: Final Project and Summary Ready to apply knowledge to large-scale or real-world projects\n"
},
{
	"uri": "http://localhost:1313/aws_ojt_report-TranThiThuHa/3-blogstranslated/",
	"title": "",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://localhost:1313/aws_ojt_report-TranThiThuHa/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://localhost:1313/aws_ojt_report-TranThiThuHa/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]